{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547482b2-29ee-4672-8eb9-ea54d7987504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !wget http://www.manythings.org/anki/rus-eng.zip -O 'data/rus-eng.zip'\n",
    "# !wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip -O 'data/en-ru.txt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c29ee8-2f16-459b-a16e-fbc96c56a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a2a0fb-666d-4e45-b95c-bf47afdf9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_anki_path = 'data/rus-eng.zip'\n",
    "# with zipfile.ZipFile(dataset_anki_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('anki_data')\n",
    "# dataset_opensub_path = 'data/en-ru.txt.zip'\n",
    "# with zipfile.ZipFile(dataset_opensub_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('opensubtitles_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503dfe70-d92f-49ed-82fc-0535f7b7b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87834a0-5e81-4ae4-8d16-0b721e279406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Zа-яёъА-ЯЁЪ.!?]+\", r\" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e96acb-86d0-40ce-b620-73db9aacd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append(normalizeString(ru_text))\n",
    "    return input_texts, target_texts\n",
    "\n",
    "def load_anki_data(file_path, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "            target_texts.append(normalizeString(ru_text)) \n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380ff032-81ee-4c4f-b646-5f83958107d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append(normalizeString(ru_text))\n",
    "    return input_texts, target_texts\n",
    "\n",
    "def load_anki_data(file_path, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "            target_texts.append(normalizeString(ru_text)) \n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c9ee38-35f4-496a-9085-e124b0db739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    anki_input_texts, anki_target_texts = load_anki_data('anki_data/rus.txt')\n",
    "    opensub_input_texts, opensub_target_texts = load_opensubtitles_data('opensubtitles_data/OpenSubtitles.en-ru.ru',\n",
    "                                                                 'opensubtitles_data/OpenSubtitles.en-ru.en')\n",
    "    pairs = list(zip(anki_input_texts, anki_target_texts)) + list(zip(opensub_input_texts, opensub_target_texts))\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56cba31-95cf-431e-9e9b-d2cb18cf5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p, n=MAX_LENGTH ):\n",
    "    return len(p[0].split(' ')) < n and \\\n",
    "        len(p[1].split(' ')) < n and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs, n=MAX_LENGTH):\n",
    "    return [pair for pair in pairs if filterPair(pair,n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72588991-911e-4ece-bd24-ee062aa48b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 123924 sentence pairs\n",
      "Trimmed to 10000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2035\n",
      "rus 4361\n",
      "('you re too humble .', 'ты слишком скромная .')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)[:10000]\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'rus', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3dc575-ed00-40a1-9234-082a7beee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "  \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "\n",
    "class AttenDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttenDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.wh = nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "        self.we = nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size,bias=False)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1,1, self.hidden_size))\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # [1,1] [1,1,512] [10,512]\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn  = torch.tanh(self.wh(hidden)+self.we(encoder_outputs.unsqueeze(0)))\n",
    "        scores = torch.bmm(self.weight,attn.transpose(1,2))\n",
    "        weights = F.log_softmax(scores,2)\n",
    "        \n",
    "        context = torch.bmm(weights,encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded, context), 2)\n",
    "        output = self.attn_combine(output)\n",
    "                           \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d6b743b-ce85-40db-9d3b-363afdb77701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ea553a-2404-4fff-9243-cb2a5e1dcd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH,teacher_forcing_ratio = 0.5):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    kc =5 + 100*(1-teacher_forcing_ratio)\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), kc)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), kc)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2c04d4-545f-474e-acfb-161a7f2ba57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a52da5-3c26-44a0-a216-ecf0f37f40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=5e-5):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter_ in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_ - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, \n",
    "                     criterion,teacher_forcing_ratio = 1-iter_/n_iters)\n",
    "        print_loss_total += loss\n",
    "        if iter_ % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter_ / n_iters),\n",
    "                                         iter_, iter_ / n_iters * 100, print_loss_avg))\n",
    "            if print_loss_avg < 0.01:\n",
    "                continue\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f2d20f-0f3a-4a3e-8ed1-bad7adf52a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "                            \n",
    "        \n",
    "\n",
    "def evaluateRandomly(encoder, decoder, l, n=3):\n",
    "    pairs_ = [pair for pair in pairs if len(pair[0].split()) == l]\n",
    "    for i in range(n):\n",
    "        \n",
    "        \n",
    "        pair = random.choice(pairs_)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3441364-b55e-42b2-8fef-89ad466fd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [pair for pair in pairs if len(pair[0].split()) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa47cd4b-2d43-47a2-b336-db315facd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttenDecoder(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e959c9f8-f019-4ed8-89f8-10ced4d229b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8m 10s (- 73m 38s) (10000 10%) 4.3371\n",
      "22m 29s (- 89m 57s) (20000 20%) 3.9063\n",
      "274m 46s (- 641m 7s) (30000 30%) 3.6611\n",
      "288m 4s (- 432m 7s) (40000 40%) 3.4898\n",
      "302m 23s (- 302m 23s) (50000 50%) 3.3222\n",
      "315m 55s (- 210m 37s) (60000 60%) 3.2385\n",
      "331m 35s (- 142m 6s) (70000 70%) 3.1433\n",
      "340m 19s (- 85m 4s) (80000 80%) 3.0372\n",
      "347m 5s (- 38m 33s) (90000 90%) 2.9325\n",
      "353m 49s (- 0m 0s) (100000 100%) 2.8550\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 100_000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0278211d-5d39-4f3d-b58e-06da35512006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> she is dead .\n",
      "= она умерла .\n",
      "< она она она она она она она она она она\n",
      "\n",
      "> we re drinking .\n",
      "= мы пьем .\n",
      "< <EOS>\n",
      "\n",
      "> you re mean .\n",
      "= вы злая .\n",
      "< . . . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 words\n",
    "evaluateRandomly(encoder1, attn_decoder1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e2b06cc-2c6c-4e3b-8ce6-5364a52cfb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m so proud of you .\n",
      "= я так тобои горжусь .\n",
      "< я я я я я я я я я я\n",
      "\n",
      "> i m blind in one eye .\n",
      "= у меня один глаз не видит .\n",
      "< . . <EOS>\n",
      "\n",
      "> i m not good at it .\n",
      "= я в этом не сильна .\n",
      "< . . . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 words\n",
    "evaluateRandomly(encoder1, attn_decoder1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26150b5f-5fa5-42d1-8f0b-7cf582080771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m at my wit s end .\n",
      "= ума не приложу что делать .\n",
      "< я я я я я я я я я я\n",
      "\n",
      "> i m at my wit s end .\n",
      "= не знаю что еще можно предпринять .\n",
      "< я я я я я я я я я я\n",
      "\n",
      "> i m as old as he is .\n",
      "= мы с ним одного возраста .\n",
      "< мы мне мне мне <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 words\n",
    "evaluateRandomly(encoder1, attn_decoder1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37de4268-6c41-452e-b22f-456d0dbe8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Слово 1 Слово 2  Косинусное расстояние       Тип\n",
      "0  страшно  ужасно               0.035930  Синонимы\n",
      "1  страшно  уверен              -0.010225  Антонимы\n"
     ]
    }
   ],
   "source": [
    "def get_word_vector(word, encoder, decoder, vocab, device='cpu'):\n",
    "    input_tensor = torch.tensor([[vocab[word]]], device=device)\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[0], encoder_hidden)\n",
    "    \n",
    "    return encoder_hidden[0].squeeze(0)\n",
    "\n",
    "def evaluate_word_pairs(encoder, decoder, vocab, synonym_pairs, antonym_pairs, device='cpu'):\n",
    "    results = []\n",
    "\n",
    "    def cosine_similarity_torch(vec1, vec2):\n",
    "        # Ensure the vectors are 2D for cosine similarity\n",
    "        if vec1.dim() == 1:\n",
    "            vec1 = vec1.unsqueeze(0)\n",
    "        if vec2.dim() == 1:\n",
    "            vec2 = vec2.unsqueeze(0)\n",
    "        \n",
    "        cos = F.cosine_similarity(vec1, vec2, dim=-1)  # Compare along the last dimension\n",
    "        return cos.item()\n",
    "\n",
    "    for word1, word2 in synonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Синонимы\"])\n",
    "\n",
    "    for word1, word2 in antonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Антонимы\"])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Слово 1\", \"Слово 2\", \"Косинусное расстояние\", \"Тип\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example word pairs\n",
    "synonym_pairs = [(\"страшно\", \"ужасно\")]\n",
    "antonym_pairs = [(\"страшно\", \"уверен\")]\n",
    "\n",
    "# Assuming encoder1, attn_decoder1, and output_lang.word2index are defined\n",
    "df_results = evaluate_word_pairs(encoder1, attn_decoder1, output_lang.word2index, synonym_pairs, antonym_pairs)\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495927a1-d2f7-4d91-a540-e2254738aacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f09717-8654-41c7-98a4-4956805bd30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dba7b7-7da1-409c-8173-04e179bb57a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b89e5-1c44-4959-a2e9-3f7660e70b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
