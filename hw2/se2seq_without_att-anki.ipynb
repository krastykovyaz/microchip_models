{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc42317-789f-49ab-bb9a-cbba886f63e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2024-09-12 08:20:27--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16305013 (16M) [application/zip]\n",
      "Saving to: ‘data/rus-eng.zip’\n",
      "\n",
      "data/rus-eng.zip    100%[===================>]  15.55M  4.00MB/s    in 5.3s    \n",
      "\n",
      "2024-09-12 08:20:33 (2.95 MB/s) - ‘data/rus-eng.zip’ saved [16305013/16305013]\n",
      "\n",
      "--2024-09-12 08:20:33--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 610036 (596K) [application/zip]\n",
      "Saving to: ‘data/en-ru.txt.zip’\n",
      "\n",
      "data/en-ru.txt.zip  100%[===================>] 595.74K  1.72MB/s    in 0.3s    \n",
      "\n",
      "2024-09-12 08:20:34 (1.72 MB/s) - ‘data/en-ru.txt.zip’ saved [610036/610036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget http://www.manythings.org/anki/rus-eng.zip -O 'data/rus-eng.zip'\n",
    "!wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip -O 'data/en-ru.txt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f6b6b3-37d8-45b1-9d92-8f93ab00e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d8d022-ec2e-4b11-90f1-116e52e5eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_anki_path = 'data/rus-eng.zip'\n",
    "# with zipfile.ZipFile(dataset_anki_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('anki_data')\n",
    "# dataset_opensub_path = 'data/en-ru.txt.zip'\n",
    "# with zipfile.ZipFile(dataset_opensub_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('opensubtitles_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4e0a59-8613-4823-ba14-ed0948c661f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8a667-62c8-4b85-8e52-9b43cc10f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Zа-яёъА-ЯЁЪ.!?]+\", r\" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e51a44-8d77-489d-852a-6707cb2d6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append(normalizeString(ru_text))\n",
    "    return input_texts, target_texts\n",
    "\n",
    "def load_anki_data(file_path, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "            target_texts.append(normalizeString(ru_text)) \n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05530c6-2939-4bd0-b576-68324ab33ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab5717-51b5-43f9-8a58-af5ac972517b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d225a3-1621-455e-bed4-dea573fabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # lines = open('anki_data/rus.txt', encoding='utf-8').\\\n",
    "    #     read().strip().split('\\n')\n",
    "\n",
    "    # pairs = [[normalizeString(s) for s in l.split('\\t')][0:2] for l in lines]\n",
    "    anki_input_texts, anki_target_texts = load_anki_data('anki_data/rus.txt')\n",
    "    opensub_input_texts, opensub_target_texts = load_opensubtitles_data('opensubtitles_data/OpenSubtitles.en-ru.ru',\n",
    "                                                                 'opensubtitles_data/OpenSubtitles.en-ru.en')\n",
    "    pairs = list(zip(anki_input_texts, anki_target_texts)) \n",
    "    # pairs = list(zip(opensub_input_texts, opensub_target_texts))\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf7f677-98ec-4e04-8155-8aab93e75a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH  and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1bfcc1-2835-453d-9771-fa0480f84a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Counted words:\n",
      "eng 2157\n",
      "rus 4609\n",
      "('i m shivering .', 'я дрожу .')\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'rus', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8781adf-7aa5-42aa-8ba0-3009736e92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size,num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if str(self.rnn)[:4] == 'LSTM':\n",
    "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "                ,torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size,num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if str(self.rnn)[:4] == 'LSTM':\n",
    "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "                ,torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e8eeb6-791f-4fe9-a34f-bdb252ecc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b945ff04-3606-4596-99e1-ec6fe66769c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    # Encode input tensor\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # Decode using the encoded hidden state\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3801ee-35e5-40bc-b836-57fe62f3b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3f507e-1b91-4b14-9731-6f24cb819a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000,\n",
    "               learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print_time = time.time()\n",
    "    print_iter = 0\n",
    "    print_loss_total = 0\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter_ in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_ - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        print_iter += 1\n",
    "\n",
    "        if iter_ % 100 == 0 : \n",
    "            if (time.time() - print_time > 30) or iter_ == n_iters:\n",
    "                print_time = time.time()\n",
    "                print_loss_avg = print_loss_total / print_iter\n",
    "                print_iter = 0\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_ / n_iters),\n",
    "                                             iter_, iter_ / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "            \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4575ca4d-c161-4a09-8f9d-82eaddac592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 31s (- 86m 1s) (600 0%) 4.1963\n",
      "1m 1s (- 84m 33s) (1200 1%) 3.6640\n",
      "1m 32s (- 83m 54s) (1800 1%) 3.5032\n",
      "2m 6s (- 82m 18s) (2500 2%) 3.3671\n",
      "2m 41s (- 81m 18s) (3200 3%) 3.2425\n",
      "3m 12s (- 81m 7s) (3800 3%) 3.2209\n",
      "3m 46s (- 80m 9s) (4500 4%) 3.1078\n",
      "4m 21s (- 79m 27s) (5200 5%) 3.0348\n",
      "4m 52s (- 79m 8s) (5800 5%) 3.0477\n",
      "5m 24s (- 79m 5s) (6400 6%) 2.9605\n",
      "5m 55s (- 78m 39s) (7000 7%) 2.9204\n",
      "6m 26s (- 78m 17s) (7600 7%) 2.8729\n",
      "6m 57s (- 77m 53s) (8200 8%) 2.7795\n",
      "7m 28s (- 77m 27s) (8800 8%) 2.7271\n",
      "7m 59s (- 77m 1s) (9400 9%) 2.8342\n",
      "8m 31s (- 76m 43s) (10000 10%) 2.6981\n",
      "9m 1s (- 76m 8s) (10600 10%) 2.7394\n",
      "9m 33s (- 75m 50s) (11200 11%) 2.6713\n",
      "10m 4s (- 75m 17s) (11800 11%) 2.6518\n",
      "10m 38s (- 75m 9s) (12400 12%) 2.6007\n",
      "11m 10s (- 74m 47s) (13000 13%) 2.5630\n",
      "11m 43s (- 74m 31s) (13600 13%) 2.5048\n",
      "12m 15s (- 74m 4s) (14200 14%) 2.5850\n",
      "12m 48s (- 73m 43s) (14800 14%) 2.5247\n",
      "13m 20s (- 73m 18s) (15400 15%) 2.5119\n",
      "13m 52s (- 72m 50s) (16000 16%) 2.4227\n",
      "14m 26s (- 72m 34s) (16600 16%) 2.4624\n",
      "14m 59s (- 72m 10s) (17200 17%) 2.3582\n",
      "15m 32s (- 71m 44s) (17800 17%) 2.3503\n",
      "16m 5s (- 71m 20s) (18400 18%) 2.2868\n",
      "16m 38s (- 70m 58s) (19000 19%) 2.2637\n",
      "17m 12s (- 70m 34s) (19600 19%) 2.3054\n",
      "17m 45s (- 70m 8s) (20200 20%) 2.2565\n",
      "18m 19s (- 69m 48s) (20800 20%) 2.2566\n",
      "18m 55s (- 69m 29s) (21400 21%) 2.1489\n",
      "19m 31s (- 69m 12s) (22000 22%) 2.1524\n",
      "20m 2s (- 69m 24s) (22400 22%) 2.1169\n",
      "20m 36s (- 69m 23s) (22900 22%) 2.1395\n",
      "21m 12s (- 69m 23s) (23400 23%) 2.0972\n",
      "21m 44s (- 69m 13s) (23900 23%) 2.1115\n",
      "22m 19s (- 69m 34s) (24300 24%) 2.1488\n",
      "55m 17s (- 168m 32s) (24700 24%) 2.0530\n",
      "70m 45s (- 214m 32s) (24800 24%) 1.9195\n",
      "86m 0s (- 259m 24s) (24900 24%) 1.9641\n",
      "102m 34s (- 307m 42s) (25000 25%) 2.1078\n",
      "120m 37s (- 359m 57s) (25100 25%) 1.9486\n",
      "136m 16s (- 404m 29s) (25200 25%) 2.0560\n",
      "153m 7s (- 452m 7s) (25300 25%) 1.8565\n",
      "169m 15s (- 497m 5s) (25400 25%) 1.8473\n",
      "186m 14s (- 544m 7s) (25500 25%) 2.0714\n",
      "201m 52s (- 586m 43s) (25600 25%) 2.0315\n",
      "234m 9s (- 676m 56s) (25700 25%) 1.8093\n",
      "251m 30s (- 723m 20s) (25800 25%) 1.9113\n",
      "260m 24s (- 745m 2s) (25900 25%) 1.9193\n",
      "260m 54s (- 731m 9s) (26300 26%) 1.8615\n",
      "261m 25s (- 710m 26s) (26900 26%) 1.9424\n",
      "261m 56s (- 690m 35s) (27500 27%) 1.8774\n",
      "262m 27s (- 671m 33s) (28100 28%) 1.9526\n",
      "262m 58s (- 653m 17s) (28700 28%) 1.9345\n",
      "263m 28s (- 635m 45s) (29300 29%) 1.8493\n",
      "263m 59s (- 618m 55s) (29900 29%) 1.8893\n",
      "264m 34s (- 600m 2s) (30600 30%) 1.7564\n",
      "265m 4s (- 584m 32s) (31200 31%) 1.8373\n",
      "265m 35s (- 569m 35s) (31800 31%) 1.8461\n",
      "266m 9s (- 552m 47s) (32500 32%) 1.7526\n",
      "266m 40s (- 538m 59s) (33100 33%) 1.7614\n",
      "267m 10s (- 525m 37s) (33700 33%) 1.6688\n",
      "267m 40s (- 512m 43s) (34300 34%) 1.7188\n",
      "268m 15s (- 498m 10s) (35000 35%) 1.6269\n",
      "268m 45s (- 486m 10s) (35600 35%) 1.6198\n",
      "269m 15s (- 474m 33s) (36200 36%) 1.6356\n",
      "269m 46s (- 463m 18s) (36800 36%) 1.6131\n",
      "270m 20s (- 450m 34s) (37500 37%) 1.5429\n",
      "270m 51s (- 440m 3s) (38100 38%) 1.6417\n",
      "271m 21s (- 429m 49s) (38700 38%) 1.5283\n",
      "271m 51s (- 419m 54s) (39300 39%) 1.5301\n",
      "272m 26s (- 408m 40s) (40000 40%) 1.5566\n",
      "272m 58s (- 399m 22s) (40600 40%) 1.4565\n",
      "273m 28s (- 390m 18s) (41200 41%) 1.4163\n",
      "273m 59s (- 381m 30s) (41800 41%) 1.4781\n",
      "274m 29s (- 372m 54s) (42400 42%) 1.4479\n",
      "275m 0s (- 364m 32s) (43000 43%) 1.3665\n",
      "275m 30s (- 356m 24s) (43600 43%) 1.4463\n",
      "276m 1s (- 348m 27s) (44200 44%) 1.4258\n",
      "276m 36s (- 339m 26s) (44900 44%) 1.4297\n",
      "277m 11s (- 330m 40s) (45600 45%) 1.4234\n",
      "277m 41s (- 323m 22s) (46200 46%) 1.3737\n",
      "278m 16s (- 315m 3s) (46900 46%) 1.3051\n",
      "278m 47s (- 308m 7s) (47500 47%) 1.3249\n",
      "279m 17s (- 301m 21s) (48100 48%) 1.3026\n",
      "279m 48s (- 294m 44s) (48700 48%) 1.3274\n",
      "280m 22s (- 287m 11s) (49400 49%) 1.2730\n",
      "280m 56s (- 280m 56s) (50000 50%) 1.2373\n",
      "281m 28s (- 274m 48s) (50600 50%) 1.2030\n",
      "282m 0s (- 268m 47s) (51200 51%) 1.2566\n",
      "282m 33s (- 262m 55s) (51800 51%) 1.1985\n",
      "283m 6s (- 257m 10s) (52400 52%) 1.2255\n",
      "283m 38s (- 251m 32s) (53000 53%) 1.2279\n",
      "284m 10s (- 245m 59s) (53600 53%) 1.2804\n",
      "284m 42s (- 240m 35s) (54200 54%) 1.2008\n",
      "285m 14s (- 235m 16s) (54800 54%) 1.2596\n",
      "285m 47s (- 230m 4s) (55400 55%) 1.1518\n",
      "286m 20s (- 224m 58s) (56000 56%) 1.1419\n",
      "286m 52s (- 219m 58s) (56600 56%) 1.1700\n",
      "287m 24s (- 215m 3s) (57200 57%) 1.1684\n",
      "287m 57s (- 210m 14s) (57800 57%) 1.1906\n",
      "288m 29s (- 205m 29s) (58400 58%) 1.1251\n",
      "289m 1s (- 200m 51s) (59000 59%) 1.1063\n",
      "289m 33s (- 196m 16s) (59600 59%) 1.1892\n",
      "290m 5s (- 191m 47s) (60200 60%) 1.1085\n",
      "290m 37s (- 187m 22s) (60800 60%) 1.0856\n",
      "291m 10s (- 183m 2s) (61400 61%) 1.0897\n",
      "291m 42s (- 178m 47s) (62000 62%) 1.1153\n",
      "292m 14s (- 174m 35s) (62600 62%) 1.1098\n",
      "292m 47s (- 170m 29s) (63200 63%) 1.0645\n",
      "293m 21s (- 166m 27s) (63800 63%) 1.0227\n",
      "293m 55s (- 162m 28s) (64400 64%) 1.0129\n",
      "294m 27s (- 158m 33s) (65000 65%) 0.9910\n",
      "295m 0s (- 154m 41s) (65600 65%) 1.0124\n",
      "295m 33s (- 150m 54s) (66200 66%) 0.9926\n",
      "296m 7s (- 147m 10s) (66800 66%) 0.9721\n",
      "296m 39s (- 143m 29s) (67400 67%) 0.9356\n",
      "297m 12s (- 139m 51s) (68000 68%) 0.9562\n",
      "297m 45s (- 136m 17s) (68600 68%) 0.9273\n",
      "298m 18s (- 132m 46s) (69200 69%) 0.9374\n",
      "298m 50s (- 129m 17s) (69800 69%) 0.9609\n",
      "299m 23s (- 125m 52s) (70400 70%) 0.9938\n",
      "299m 55s (- 122m 30s) (71000 71%) 0.9252\n",
      "300m 28s (- 119m 10s) (71600 71%) 0.9918\n",
      "301m 0s (- 115m 54s) (72200 72%) 0.8696\n",
      "301m 32s (- 112m 39s) (72800 72%) 0.9393\n",
      "302m 4s (- 109m 28s) (73400 73%) 0.9394\n",
      "302m 37s (- 105m 46s) (74100 74%) 0.8527\n",
      "303m 9s (- 102m 8s) (74800 74%) 0.8675\n",
      "303m 42s (- 98m 33s) (75500 75%) 0.8858\n",
      "304m 15s (- 95m 1s) (76200 76%) 0.9018\n",
      "304m 48s (- 91m 33s) (76900 76%) 0.8780\n",
      "305m 22s (- 88m 39s) (77500 77%) 0.8130\n",
      "305m 55s (- 85m 47s) (78100 78%) 0.8687\n",
      "306m 29s (- 82m 56s) (78700 78%) 0.8362\n",
      "307m 2s (- 80m 8s) (79300 79%) 0.8327\n",
      "307m 36s (- 77m 22s) (79900 79%) 0.8563\n",
      "308m 9s (- 74m 38s) (80500 80%) 0.8338\n",
      "308m 42s (- 71m 56s) (81100 81%) 0.8450\n",
      "309m 15s (- 69m 16s) (81700 81%) 0.7944\n",
      "309m 48s (- 66m 37s) (82300 82%) 0.8116\n",
      "310m 21s (- 64m 1s) (82900 82%) 0.8504\n",
      "311m 36s (- 62m 1s) (83400 83%) 0.8286\n",
      "312m 10s (- 59m 54s) (83900 83%) 0.7209\n",
      "312m 42s (- 57m 21s) (84500 84%) 0.8227\n",
      "313m 14s (- 54m 50s) (85100 85%) 0.8080\n",
      "313m 46s (- 52m 21s) (85700 85%) 0.8003\n",
      "314m 17s (- 49m 53s) (86300 86%) 0.7146\n",
      "314m 48s (- 47m 27s) (86900 86%) 0.7833\n",
      "315m 19s (- 45m 2s) (87500 87%) 0.7830\n",
      "315m 51s (- 42m 39s) (88100 88%) 0.7733\n",
      "316m 21s (- 40m 42s) (88600 88%) 0.6798\n",
      "316m 53s (- 38m 46s) (89100 89%) 0.6961\n",
      "317m 28s (- 36m 50s) (89600 89%) 0.7890\n",
      "318m 2s (- 34m 56s) (90100 90%) 0.7437\n",
      "318m 38s (- 33m 3s) (90600 90%) 0.7453\n",
      "319m 14s (- 31m 11s) (91100 91%) 0.7099\n",
      "319m 45s (- 29m 42s) (91500 91%) 0.7449\n",
      "320m 16s (- 28m 13s) (91900 91%) 0.6931\n",
      "320m 52s (- 26m 0s) (92500 92%) 0.7413\n",
      "321m 27s (- 23m 49s) (93100 93%) 0.7211\n",
      "322m 4s (- 21m 39s) (93700 93%) 0.7473\n",
      "322m 38s (- 19m 30s) (94300 94%) 0.7404\n",
      "323m 11s (- 17m 22s) (94900 94%) 0.7457\n",
      "323m 46s (- 15m 15s) (95500 95%) 0.7394\n",
      "324m 20s (- 13m 9s) (96100 96%) 0.7103\n",
      "324m 53s (- 11m 5s) (96700 96%) 0.6919\n",
      "325m 26s (- 9m 1s) (97300 97%) 0.6965\n",
      "325m 56s (- 6m 59s) (97900 97%) 0.7281\n",
      "326m 26s (- 4m 58s) (98500 98%) 0.6697\n",
      "327m 2s (- 2m 38s) (99200 99%) 0.6925\n",
      "327m 34s (- 0m 19s) (99900 99%) 0.6686\n",
      "327m 39s (- 0m 0s) (100000 100%) 0.6019\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size,1).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words,1).to(device)\n",
    "trainIters(encoder1, decoder1, 100_000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5ffcb8-fffa-44e8-ba1e-571b85baa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words\n",
    "            \n",
    "def evaluateRandomly(encoder, decoder, l, n=3):\n",
    "    pairs_ = [pair for pair in pairs if len(pair[0].split()) == l]\n",
    "    for i in range(n):\n",
    "        \n",
    "        \n",
    "        pair = random.choice(pairs_)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[1:-1])\n",
    "        print('<', output_sentence)\n",
    "        print('')             \n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbff403a-d6ed-4813-915c-0fe975f196d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> they re allies .\n",
      "= они союзницы .\n",
      "< союзники .\n",
      "\n",
      "> i m yours .\n",
      "= я твоя .\n",
      "< твоя .\n",
      "\n",
      "> i m prejudiced .\n",
      "= я пристрастен .\n",
      "< пристрастен .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 words\n",
    "evaluateRandomly(encoder1, decoder1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47dcdd72-eb0e-49a6-ace4-f6244a82cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m glad that we won .\n",
      "= я рад что мы победили .\n",
      "< рад что мы выиграли .\n",
      "\n",
      "> i m going to be busy .\n",
      "= я буду занята .\n",
      "< буду занята .\n",
      "\n",
      "> i m sure it ll pass .\n",
      "= я уверен что это проидет .\n",
      "< уверен что это уверен .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 words\n",
    "evaluateRandomly(encoder1, decoder1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66f018a-6e4f-4232-9729-00e50417fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m glad i m a man .\n",
      "= я рад что я мужчина .\n",
      "< рад что я мужчина .\n",
      "\n",
      "> i m as tall as he is .\n",
      "= я такои же высокии как и он .\n",
      "< такои же высокии как и он .\n",
      "\n",
      "> i m glad i m not tom .\n",
      "= я рад что я не том .\n",
      "< рад что я не том .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 words\n",
    "evaluateRandomly(encoder1, decoder1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b321768-e062-43d0-b6e6-2bc3442d82be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Слово 1 Слово 2  Косинусное расстояние       Тип\n",
      "0  толстыи  жирныи              -0.014418  Синонимы\n",
      "1  толстыи   худои              -0.004130  Антонимы\n"
     ]
    }
   ],
   "source": [
    "def get_word_vector(word, encoder, decoder, vocab, device='cpu'):\n",
    "    input_tensor = torch.tensor([[vocab[word]]], device=device)\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[0], encoder_hidden)\n",
    "    \n",
    "    return encoder_hidden[0].squeeze(0)\n",
    "\n",
    "def evaluate_word_pairs(encoder, decoder, vocab, synonym_pairs, antonym_pairs, device='cpu'):\n",
    "    results = []\n",
    "\n",
    "    def cosine_similarity_torch(vec1, vec2):\n",
    "        cos = F.cosine_similarity(vec1, vec2)\n",
    "        return cos.item()\n",
    "\n",
    "    for word1, word2 in synonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Синонимы\"])\n",
    "\n",
    "    for word1, word2 in antonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Антонимы\"])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Слово 1\", \"Слово 2\", \"Косинусное расстояние\", \"Тип\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "synonym_pairs = [(\"толстыи\", \"жирныи\")]\n",
    "antonym_pairs = [(\"толстыи\", \"худои\")]\n",
    "\n",
    "df_results = evaluate_word_pairs(encoder1, decoder1, output_lang.word2index, synonym_pairs, antonym_pairs)\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d55fa34-9554-4021-8691-dbd8339bd137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda39773-93e5-4efb-a5a3-6d5aa0825f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
