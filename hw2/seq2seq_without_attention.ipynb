{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0f83e6-4728-420e-8df5-a45b48b172de",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7a6f89-5983-46b1-96e8-ad6cd8409974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-08 16:10:28--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16305013 (16M) [application/zip]\n",
      "Saving to: ‘data/rus-eng.zip’\n",
      "\n",
      "data/rus-eng.zip    100%[===================>]  15.55M   687KB/s    in 24s     \n",
      "\n",
      "2024-09-08 16:10:52 (672 KB/s) - ‘data/rus-eng.zip’ saved [16305013/16305013]\n",
      "\n",
      "--2024-09-08 16:10:52--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 610036 (596K) [application/zip]\n",
      "Saving to: ‘en-ru.txt.zip’\n",
      "\n",
      "en-ru.txt.zip       100%[===================>] 595.74K  2.55MB/s    in 0.2s    \n",
      "\n",
      "2024-09-08 16:10:52 (2.55 MB/s) - ‘en-ru.txt.zip’ saved [610036/610036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/rus-eng.zip -O 'data/rus-eng.zip'\n",
    "!wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip -O 'data/en-ru.txt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2390cf4-7c3a-42ff-a0dc-ad53e21c62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0d0c7-124d-489e-8d0f-a56171419c72",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e436d59-8726-471b-ba4d-0c8be04942c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_anki_path = 'data/rus-eng.zip'\n",
    "with zipfile.ZipFile(dataset_anki_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('anki_data')\n",
    "dataset_opensub_path = 'data/en-ru.txt.zip'\n",
    "with zipfile.ZipFile(dataset_opensub_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('opensubtitles_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159eb2af-7cd1-4a3d-b360-96e6c0e99ee4",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3039b608-d2bb-4c68-8265-7a88448ae99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anki_data(file_path, num_samples=100000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(en_text)\n",
    "            target_texts.append('\\t' + ru_text + '\\n') \n",
    "    return input_texts, target_texts\n",
    "\n",
    "anki_input_texts, anki_target_texts = load_anki_data('anki_data/rus.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc784a73-270e-4b68-9c21-9f9c401c3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(en_text)\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append('\\t' + ru_text + '\\n')\n",
    "    return input_texts, target_texts\n",
    "\n",
    "opensub_input_texts, opensub_target_texts = load_opensubtitles_data('opensubtitles_data/OpenSubtitles.en-ru.ru',\n",
    "                                                                   'opensubtitles_data/OpenSubtitles.en-ru.en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d03d9a-788b-498b-8326-b616cf432a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LOS ANGELES 2029 A. D.', '\\t2029 год нашей эры.\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub_input_texts[0], opensub_target_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb205ecd-1ec3-4989-87da-669d3cc51fd5",
   "metadata": {},
   "source": [
    "### tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b539d39-623f-4312-becd-f7a8f8b4fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsaic_english_tokenizer(text: str)->List[str]:\n",
    "    return re.findall(r'w+', text.lower())\n",
    "\n",
    "def build_vocab(sentences: str, tokenizer)->Dict[str, str]:\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenizer(sentence))\n",
    "    vocab = {word: i for i, (word, _) in enumerate(counter.items(), 4)}\n",
    "    vocab['<unk>'] = 0\n",
    "    vocab['<pad>'] = 1\n",
    "    vocab['<bos>'] = 2\n",
    "    vocab['<eos>'] = 3\n",
    "    return vocab\n",
    "\n",
    "def tokenizer_sentences(sentences, vocab, tokenizer):\n",
    "    return [[vocab['<bos>'] + [vocab.get(token, vocab['<unk>']) for token in tokenizer(sentence)] +\\\n",
    "             vocab['<eos>']] for sentence in sentences]\n",
    "\n",
    "def pad_sequences(sequence, padding_value):\n",
    "    return pad_sequence([torch.tensor(seq) for seq in sequence], \\\n",
    "                        padding_value=padding_value,\n",
    "                        batch_first=True)\n",
    "    \n",
    "tokenizer = basic_english_tokenizer\n",
    "\n",
    "opensub_vocab_input = build_vocab(opensub_input_texts, tokenizer)\n",
    "opensub_vocab_target = build_vocab(opensub_target_texts, tokenizer)\n",
    "\n",
    "anki_vocab_input = build_vocab(anki_input_texts, tokenizer)\n",
    "anki_vocab_target = build_vocab(anki_target_texts, tokenizer)\n",
    "\n",
    "opensub_input_sequences = tokenize_sentences(opensub_input_texts, opensub_vocab_input, tokenizer)\n",
    "opensub_target_sequences = tokenize_sentences(opensub_target_texts, opensub_vocab_target, tokenizer)\n",
    "\n",
    "anki_input_sequences = tokenize_sentences(anki_input_texts, anki_vocab_input, tokenizer)\n",
    "anki_target_sequences = tokenize_sentences(anki_target_texts, anki_vocab_target, tokenizer)\n",
    "\n",
    "opensub_input_padded = pad_sequences(opensub_input_sequences, opensub_vocab_input['<pad>'])\n",
    "opensub_target_padded = pad_sequences(opensub_target_sequences, opensub_vocab_target['<pad>'])\n",
    "\n",
    "anki_input_padded = pad_sequences(anki_input_sequences, anki_vocab_input['<pad>'])\n",
    "anki_target_padded = pad_sequences(anki_target_sequences, anki_vocab_target['<pad>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac9e636-64b9-4940-b2da-d7b282931ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import re\n",
    "\n",
    "# # Простой токенизатор на основе регулярных выражений\n",
    "# def basic_english_tokenizer(text):\n",
    "#     return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# # Создание словаря\n",
    "# def build_vocab(sentences, tokenizer):\n",
    "#     counter = Counter()\n",
    "#     for sentence in sentences:\n",
    "#         counter.update(tokenizer(sentence))\n",
    "#     vocab = {word: i for i, (word, _) in enumerate(counter.items(), 4)}\n",
    "#     vocab['<unk>'] = 0\n",
    "#     vocab['<pad>'] = 1\n",
    "#     vocab['<bos>'] = 2\n",
    "#     vocab['<eos>'] = 3\n",
    "#     return vocab\n",
    "\n",
    "# # Токенизация предложений с использованием созданного словаря\n",
    "# def tokenize_sentences(sentences, vocab, tokenizer):\n",
    "#     return [[vocab['<bos>']] + [vocab.get(token, vocab['<unk>']) for token in tokenizer(sentence)] + [vocab['<eos>']] for sentence in sentences]\n",
    "\n",
    "# # Паддинг последовательностей\n",
    "# def pad_sequences(sequences, padding_value):\n",
    "#     return pad_sequence([torch.tensor(seq) for seq in sequences], padding_value=padding_value, batch_first=True)\n",
    "\n",
    "\n",
    "# # Создание словарей для обоих наборов данных\n",
    "# tokenizer = basic_english_tokenizer\n",
    "\n",
    "# opensub_vocab_input = build_vocab(opensub_input_texts, tokenizer)\n",
    "# opensub_vocab_target = build_vocab(opensub_target_texts, tokenizer)\n",
    "\n",
    "# anki_vocab_input = build_vocab(anki_input_texts, tokenizer)\n",
    "# anki_vocab_target = build_vocab(anki_target_texts, tokenizer)\n",
    "\n",
    "# # Токенизация данных\n",
    "# opensub_input_sequences = tokenize_sentences(opensub_input_texts, opensub_vocab_input, tokenizer)\n",
    "# opensub_target_sequences = tokenize_sentences(opensub_target_texts, opensub_vocab_target, tokenizer)\n",
    "\n",
    "# anki_input_sequences = tokenize_sentences(anki_input_texts, anki_vocab_input, tokenizer)\n",
    "# anki_target_sequences = tokenize_sentences(anki_target_texts, anki_vocab_target, tokenizer)\n",
    "\n",
    "# # Паддинг последовательностей\n",
    "# opensub_input_padded = pad_sequences(opensub_input_sequences, opensub_vocab_input['<pad>'])\n",
    "# opensub_target_padded = pad_sequences(opensub_target_sequences, opensub_vocab_target['<pad>'])\n",
    "\n",
    "# anki_input_padded = pad_sequences(anki_input_sequences, anki_vocab_input['<pad>'])\n",
    "# anki_target_padded = pad_sequences(anki_target_sequences, anki_vocab_target['<pad>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713bc96f-1ad1-4c05-9919-3700f0eb2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_size):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hidden, cell):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "#         output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4ccd14f-34fc-4ce4-beb4-c1708be9cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input_, hidden, cell):\n",
    "#         embedded = self.embedding(input_).view(1, 1, -1) # (1, 1, hidden_size)\n",
    "#         output, (hidden_cell) = self.lstm(embedded, (hidden, cell))\n",
    "#         return hidden, cell\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.zeros(1,1, self.hidden_size), # hidden state\n",
    "#                 torch.zeros(1,1,self.hidden_size)) # cell state\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_state):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input_, hidden, cell):\n",
    "#         embedded = self.embedding(input_).view(1,1,-1) # (1, 1, hidden_size)\n",
    "#         output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden, cell\n",
    "        \n",
    "  \n",
    "\n",
    "# def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "#     encoder_hidden, encoder_cell = encoder.init_hidden()\n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "#     loss = 0\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "#     decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])\n",
    "#     decoder_hidden, decoder_cell = encoder_hidden, encoder_cell # init decoder\n",
    "#     for di in range(target_length):\n",
    "#         decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "#         _, topi = decoder_output.topk(1)\n",
    "#         decoder_input = topi.squeeze().detach()\n",
    "#         loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "#         if decoder_input.item() == opensub_vocab_target['<eos>']:\n",
    "#             break\n",
    "#     loss.backward()\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "#     return loss.item() / target_length\n",
    "\n",
    "# hidden_size = 256\n",
    "# learning_rate = 0.01\n",
    "# n_iters = 10_000\n",
    "# print_every = 1000\n",
    "\n",
    "# encoder = Encoder(len(opensub_vocab_input), hidden_size)\n",
    "# decoder = Decoder(len(opensub_vocab_target), hidden_size) \n",
    "\n",
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# for iter_ in range(1, n_iters+1):\n",
    "#     training_pair = [opensub_input_padded[iter_ % len(opensub_input_padded)],\n",
    "#                      opensub_target_padded[iter_ % len(opensub_target_padded)]]\n",
    "#     input_tensor = training_pair[0]\n",
    "#     target_tensor = training_pair[1]\n",
    "#     loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10)\n",
    "#     if iter_ % print_every == 0:\n",
    "#         print(f'Iteration {iter}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fc86e-215f-4387-b230-99339f1b1dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden, encoder_cell = encoder.init_hidden()  # Initialize hidden and cell states\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode input tensor\n",
    "    for ei in range(input_length):\n",
    "        encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "\n",
    "    decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])  # Start token\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell  # Initialize decoder states with encoder states\n",
    "\n",
    "    # Decode using the encoded hidden state\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Use predicted token as next input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))  # Calculate loss\n",
    "        if decoder_input.item() == opensub_vocab_target['<eos>']:  # Stop if EOS token is predicted\n",
    "            break\n",
    "\n",
    "    loss.backward()  # Backpropagation\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_, hidden, cell):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        return hidden, cell\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),  # hidden state\n",
    "                torch.zeros(1, 1, self.hidden_size))  # cell state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_, hidden, cell):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "# Parameters\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "n_iters = 10000\n",
    "print_every = 1000\n",
    "\n",
    "# Create models\n",
    "encoder = Encoder(len(opensub_vocab_input), hidden_size)\n",
    "decoder = Decoder(len(opensub_vocab_target), hidden_size)\n",
    "\n",
    "# Optimizers and loss function\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Training loop\n",
    "for iter_ in range(1, n_iters + 1):\n",
    "    training_pair = [opensub_input_padded[iter_ % len(opensub_input_padded)], opensub_target_padded[iter_ % len(opensub_target_padded)]]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10)\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print(f'Iteration {iter_}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b7398-f79a-489c-a79a-19bda6f7970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensub_vocab_target_reverse = {index: word for word, index in opensub_vocab_target.items()}\n",
    "opensub_vocab_input_reverse = {index: word for word, index in opensub_vocab_input.items()}\n",
    "\n",
    "def translate_sentence(input_tensor, encoder, decoder, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = encoder.init_hidden()\n",
    "        input_length = input_tensor.size(0)\n",
    "        for ei in range(input_length):\n",
    "            encoder_hidden, decoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "        decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        decoder_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            if topi.item() == opensub_vocab_target['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(opensub_vocab_target_reverse[topi.item()])\n",
    "        return decoded_words\n",
    "                                                                   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ccd51e8-7b87-48fd-abe3-9f012ef98c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence of 3 words: <bos> lt would be fought here ln our present <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> он\n",
      "\n",
      "Original sentence of 6 words: <bos> what the hell <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> он\n",
      "\n",
      "Original sentence of 10 words: <bos> hey what s wrong with this picture <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> он\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создаем обратный словарь для целевого языка\n",
    "opensub_vocab_target_reverse = {index: word for word, index in opensub_vocab_target.items()}\n",
    "opensub_vocab_input_reverse = {index: word for word, index in opensub_vocab_input.items()}\n",
    "\n",
    "\n",
    "# Функция для выполнения перевода одного предложения\n",
    "def translate_sentence(input_tensor, encoder, decoder, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = encoder.init_hidden()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "\n",
    "        decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == opensub_vocab_target['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(opensub_vocab_target_reverse[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "# Пример для предложений разной длины\n",
    "sentence_lengths = [3, 6, 10]\n",
    "for length in sentence_lengths:\n",
    "    input_sentence = opensub_input_padded[length]  # Предложение из определенного количества слов\n",
    "    translated_sentence = translate_sentence(input_sentence, encoder, decoder, max_length=length)\n",
    "    print(f'Original sentence of {length} words: {\" \".join([opensub_vocab_input_reverse[word.item()] for word in input_sentence])}')\n",
    "    print(f'Translated sentence: {\" \".join(translated_sentence)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27837408-b784-43f2-aa5b-ce3c0d099167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM vector for 'happy': tensor([[ 2.3426e-03, -4.0188e-02,  1.4073e-02, -1.3712e-01, -1.8990e-01,\n",
      "         -1.5245e-01,  2.8612e-02,  5.2273e-02, -2.2459e-02, -2.6216e-02,\n",
      "         -2.6487e-01,  2.6727e-02, -3.0794e-01,  7.8054e-02,  3.4654e-02,\n",
      "         -1.2907e-01, -9.2686e-02,  1.2740e-01, -1.0494e-01,  2.7748e-02,\n",
      "          1.1791e-01, -1.5569e-01,  1.3155e-01,  9.1160e-02,  4.3471e-02,\n",
      "         -4.5930e-02, -2.2109e-01,  5.1826e-02,  7.6673e-02,  1.0606e-01,\n",
      "         -1.5880e-01, -1.3279e-01,  6.8736e-02,  1.7262e-02, -1.3951e-01,\n",
      "         -7.9645e-02, -7.7770e-02, -6.6938e-02, -6.5604e-02, -7.3534e-02,\n",
      "          1.1132e-01,  1.4896e-04, -1.4683e-01, -1.0186e-01, -8.8630e-02,\n",
      "          1.5370e-01, -1.6466e-01,  2.8312e-02,  1.8305e-01, -6.5646e-02,\n",
      "          4.0294e-02,  1.0834e-01,  1.3756e-02,  1.6939e-01, -3.9089e-02,\n",
      "         -5.6647e-02, -5.4708e-02,  2.0035e-01, -1.5366e-01,  9.9880e-02,\n",
      "          1.8550e-01,  5.2398e-02,  1.0620e-01,  1.6688e-01,  1.1317e-01,\n",
      "          1.2782e-01,  1.0819e-01,  1.5059e-02,  9.8142e-02,  1.4498e-01,\n",
      "          1.3755e-01,  9.7152e-02,  6.8217e-02, -4.9468e-02, -1.1542e-01,\n",
      "         -1.2217e-01,  3.5900e-02,  1.2075e-02,  1.7143e-01,  1.5729e-01,\n",
      "          1.4432e-02,  1.0651e-01,  8.8040e-02, -1.1824e-01, -1.1927e-01,\n",
      "         -1.0766e-01, -4.7789e-02, -6.4801e-02,  1.4661e-01,  1.3109e-01,\n",
      "          9.8950e-02, -7.0975e-02,  6.3646e-02,  6.0134e-02, -1.8702e-01,\n",
      "          1.0186e-01,  6.3362e-02, -2.7642e-02,  5.9142e-02, -5.7810e-02,\n",
      "          4.8088e-02,  2.0513e-02,  6.2274e-03, -6.2079e-02,  8.1748e-02,\n",
      "         -2.8751e-01,  3.7456e-02, -1.6732e-02,  5.9174e-02,  7.1452e-02,\n",
      "          5.2556e-02, -1.0800e-02, -6.1513e-02, -2.4115e-01, -5.4397e-02,\n",
      "         -5.2915e-02, -1.0847e-01,  8.9882e-02,  3.7021e-02,  1.1649e-01,\n",
      "         -2.0770e-01, -4.8829e-02,  5.2646e-02, -1.2511e-01, -1.5265e-01,\n",
      "          8.4840e-02, -1.3389e-01,  1.5251e-01,  1.3285e-01,  9.4202e-02,\n",
      "         -1.1073e-01, -1.8465e-01,  1.9856e-01,  2.5289e-01,  3.5392e-02,\n",
      "         -2.0978e-01, -1.5984e-01, -3.8119e-02,  3.0392e-02,  2.2252e-01,\n",
      "          5.2133e-03, -1.1907e-01, -1.0645e-01,  1.6166e-02, -7.2804e-02,\n",
      "          1.0915e-01, -1.0363e-01,  3.7538e-02,  1.8137e-01, -2.1284e-01,\n",
      "         -1.1308e-01, -1.3588e-01, -7.4495e-02, -1.7783e-02,  1.3575e-01,\n",
      "         -3.3628e-02,  1.5250e-01,  6.1542e-02,  1.5533e-02, -7.2572e-02,\n",
      "          5.2977e-02, -2.5600e-02, -1.7374e-01, -4.1540e-02,  5.5559e-02,\n",
      "          1.1254e-01,  1.5279e-01, -3.7027e-02,  1.4611e-01, -6.2094e-02,\n",
      "          5.3989e-02, -2.6161e-01,  1.0114e-01, -2.4970e-01, -2.1143e-02,\n",
      "          3.2695e-02,  2.2789e-01,  1.3579e-01,  2.7116e-01,  6.1931e-02,\n",
      "         -2.1227e-01,  3.4875e-02, -6.3064e-02,  1.8007e-01, -3.8664e-02,\n",
      "          2.0864e-01,  9.3823e-02, -1.0621e-01, -8.6764e-02,  5.6336e-02,\n",
      "          6.3504e-02, -1.4509e-02, -1.9533e-01,  1.5176e-01,  4.4215e-02,\n",
      "          8.6973e-02, -1.1676e-01, -3.3782e-02, -2.0314e-01,  4.5557e-02,\n",
      "         -3.9670e-02,  8.4336e-03,  4.7730e-02, -3.7524e-02, -7.2778e-02,\n",
      "          1.0439e-02, -1.0944e-01,  1.5596e-01, -7.5653e-02, -1.1019e-01,\n",
      "         -3.2440e-02, -1.5625e-01, -2.1079e-01, -8.4729e-03,  6.2266e-02,\n",
      "         -1.4792e-01, -1.6743e-02,  8.2454e-02,  3.6797e-02,  7.9872e-03,\n",
      "         -1.2951e-02, -1.1878e-01, -6.2048e-02, -9.5008e-02, -7.0264e-02,\n",
      "          5.1253e-02, -3.7837e-02, -1.7906e-01,  2.1745e-02,  1.3785e-01,\n",
      "          2.9837e-02,  1.1938e-01,  8.0610e-02,  3.8746e-02,  1.6347e-01,\n",
      "          3.1217e-01, -1.0406e-02,  1.6681e-01, -6.1746e-03,  5.4255e-02,\n",
      "         -3.3736e-02,  1.0313e-01, -1.9627e-01, -1.2595e-01,  7.2688e-02,\n",
      "         -2.1631e-02, -4.5186e-02,  2.0712e-02, -1.3470e-01,  1.9640e-01,\n",
      "         -8.7753e-02, -8.9550e-03,  6.4021e-02, -2.4994e-01,  1.6781e-02,\n",
      "          4.8202e-02]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_word_vector_lstm(word, encoder, vocab, device='cpu'):\n",
    "    # Преобразование слова в тензор\n",
    "    input_tensor = torch.tensor([vocab[word]]).to(device)\n",
    "    \n",
    "    # Получение эмбеддинга слова\n",
    "    embedded = encoder.embedding(input_tensor)\n",
    "    \n",
    "    # Инициализация скрытых состояний\n",
    "    hidden, cell = encoder.init_hidden()\n",
    "    \n",
    "    # Прямой проход через LSTM\n",
    "    embedded = embedded.view(1, 1, -1)  # (1, 1, hidden_size) чтобы соответствовать формату LSTM\n",
    "    \n",
    "    # Проверка и исправление размерностей для LSTM\n",
    "    if hidden.dim() != 3:\n",
    "        hidden = hidden.unsqueeze(0)  # Приводим к размерности (num_layers, batch_size, hidden_size)\n",
    "    if cell.dim() != 3:\n",
    "        cell = cell.unsqueeze(0)  # Приводим к размерности (num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    output, (hidden, cell) = encoder.lstm(embedded, (hidden, cell))\n",
    "    \n",
    "    # Возвращаем последнее скрытое состояние\n",
    "    return hidden[-1]  # Возвращаем последнюю скрытую вектору\n",
    "\n",
    "\n",
    "vocab = opensub_vocab_input  # Например, словарь для кодировщика\n",
    "word = 'happy'\n",
    "\n",
    "# Получение векторов слов\n",
    "vector_lstm = get_word_vector_lstm(word, encoder, vocab)\n",
    "\n",
    "print(f\"LSTM vector for '{word}': {vector_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9763d75-7996-4bf6-b602-09571e5b35fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
