{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f18523-3e86-4e4d-b834-404b4f69c9b9",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7a6f89-5983-46b1-96e8-ad6cd8409974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-08 16:10:28--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16305013 (16M) [application/zip]\n",
      "Saving to: ‘data/rus-eng.zip’\n",
      "\n",
      "data/rus-eng.zip    100%[===================>]  15.55M   687KB/s    in 24s     \n",
      "\n",
      "2024-09-08 16:10:52 (672 KB/s) - ‘data/rus-eng.zip’ saved [16305013/16305013]\n",
      "\n",
      "--2024-09-08 16:10:52--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 610036 (596K) [application/zip]\n",
      "Saving to: ‘en-ru.txt.zip’\n",
      "\n",
      "en-ru.txt.zip       100%[===================>] 595.74K  2.55MB/s    in 0.2s    \n",
      "\n",
      "2024-09-08 16:10:52 (2.55 MB/s) - ‘en-ru.txt.zip’ saved [610036/610036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/rus-eng.zip -O 'data/rus-eng.zip'\n",
    "!wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip -O 'data/en-ru.txt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2390cf4-7c3a-42ff-a0dc-ad53e21c62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3be48-6b9c-4a1b-bd3e-12840490b1ae",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b2fd6ad-c1a7-4ddd-b224-6877a097f36e",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manki_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m dataset_opensub_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/en-ru.txt.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_opensub_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      6\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopensubtitles_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/zipfile.py:1269\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/zipfile.py:1336\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "dataset_anki_path = 'data/rus-eng.zip'\n",
    "with zipfile.ZipFile(dataset_anki_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('anki_data')\n",
    "dataset_opensub_path = 'data/en-ru.txt.zip'\n",
    "with zipfile.ZipFile(dataset_opensub_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('opensubtitles_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56a9bf-a7b7-49b9-a7ac-10ff1aa8124d",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039b608-d2bb-4c68-8265-7a88448ae99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anki_data(file_path, num_samples=100000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(en_text)\n",
    "            target_texts.append('\\t' + ru_text + '\\n') \n",
    "    return input_texts, target_texts\n",
    "\n",
    "anki_input_texts, anki_target_texts = load_anki_data('anki_data/rus.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fb325-0ad9-4cbb-ba79-bbe84a979e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc784a73-270e-4b68-9c21-9f9c401c3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(en_text)\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append('\\t' + ru_text + '\\n')\n",
    "    return input_texts, target_texts\n",
    "\n",
    "opensub_input_texts, opensub_target_texts = load_opensubtitles_data('opensubtitles_data/OpenSubtitles.en-ru.ru',\n",
    "                                                                   'opensubtitles_data/OpenSubtitles.en-ru.en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47d03d9a-788b-498b-8326-b616cf432a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LOS ANGELES 2029 A. D.', '\\t2029 год нашей эры.\\n')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub_input_texts[0], opensub_target_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34c424-026d-4b0d-80db-495ef497ec89",
   "metadata": {},
   "source": [
    "### tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2dacf30-6fb8-45ad-bf15-f88d07d8e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_english_tokenizer(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def build_vocab(sentences, tokenizer):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenizer(sentence))\n",
    "    vocab = {word: i for i, (word, _) in enumerate(counter.items(), 4)}\n",
    "    vocab['<unk>'] = 0\n",
    "    vocab['<pad>'] = 1\n",
    "    vocab['<bos>'] = 2\n",
    "    vocab['<eos>'] = 3\n",
    "    return vocab\n",
    "\n",
    "def tokenizer_sentences(sentences, vocab, tokenizer):\n",
    "    return [[vocab['<bos>']] + [vocab.get(token, vocab['<unk>']) for token in tokenizer(sentence)]  + \\\n",
    "            [vocab['<eos>']] for sentence in sentences]\n",
    "\n",
    "def pad_sentences(sequences, padding_value):\n",
    "    return pad_sequence([torch.tensor(seq) for seq in sequences], padding_value=padding_value, batch_first=True)\n",
    "\n",
    "tokenizer = basic_english_tokenizer\n",
    "\n",
    "opensub_vocab_input = build_vocab(opensub_input_texts, tokenizer)\n",
    "opensub_vocab_target = build_vocab(opensub_target_texts, tokenizer)\n",
    "# anki_vocab_input = build_vocab(anki_input_texts, tokenizer)\n",
    "# anki_vocab_target = build_vocab(anki_target_texts, tokenizer)\n",
    "\n",
    "opensub_input_sequences = tokenize_sentences(opensub_input_texts, opensub_vocab_input, tokenizer)\n",
    "opensub_target_sequences = tokenize_sentences(opensub_target_texts, opensub_vocab_target, tokenizer)\n",
    "\n",
    "# anki_input_sequences = tokenize_sentences(anki_input_texts, anki_vocab_input, tokenizer)\n",
    "# anki_target_sequences = tokenize_sentences(anki_target_texts, anki_vocab_target, tokenizer)\n",
    "\n",
    "opensub_input_padded = pad_sentences(opensub_input_sequences, opensub_vocab_input['<pad>'])\n",
    "opensub_target_padded = pad_sentences(opensub_target_sequences, opensub_vocab_input['<pad>'])\n",
    "\n",
    "# anki_input_padded = pad_sentences(anki_input_sequences, anki_vocab_input['<pad>'])\n",
    "# anki_target_padded = pad_sentences(anki_target_sequences, anki_vocab_input['<pad>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eac9e636-64b9-4940-b2da-d7b282931ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import re\n",
    "\n",
    "# # Простой токенизатор на основе регулярных выражений\n",
    "# def basic_english_tokenizer(text):\n",
    "#     return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# # Создание словаря\n",
    "# def build_vocab(sentences, tokenizer):\n",
    "#     counter = Counter()\n",
    "#     for sentence in sentences:\n",
    "#         counter.update(tokenizer(sentence))\n",
    "#     vocab = {word: i for i, (word, _) in enumerate(counter.items(), 4)}\n",
    "#     vocab['<unk>'] = 0\n",
    "#     vocab['<pad>'] = 1\n",
    "#     vocab['<bos>'] = 2\n",
    "#     vocab['<eos>'] = 3\n",
    "#     return vocab\n",
    "\n",
    "# # Токенизация предложений с использованием созданного словаря\n",
    "# def tokenize_sentences(sentences, vocab, tokenizer):\n",
    "#     return [[vocab['<bos>']] + [vocab.get(token, vocab['<unk>']) for token in tokenizer(sentence)] + [vocab['<eos>']] for sentence in sentences]\n",
    "\n",
    "# # Паддинг последовательностей\n",
    "# def pad_sequences(sequences, padding_value):\n",
    "#     return pad_sequence([torch.tensor(seq) for seq in sequences], padding_value=padding_value, batch_first=True)\n",
    "\n",
    "\n",
    "# # Создание словарей для обоих наборов данных\n",
    "# tokenizer = basic_english_tokenizer\n",
    "\n",
    "# opensub_vocab_input = build_vocab(opensub_input_texts, tokenizer)\n",
    "# opensub_vocab_target = build_vocab(opensub_target_texts, tokenizer)\n",
    "\n",
    "# anki_vocab_input = build_vocab(anki_input_texts, tokenizer)\n",
    "# anki_vocab_target = build_vocab(anki_target_texts, tokenizer)\n",
    "\n",
    "# # Токенизация данных\n",
    "# opensub_input_sequences = tokenize_sentences(opensub_input_texts, opensub_vocab_input, tokenizer)\n",
    "# opensub_target_sequences = tokenize_sentences(opensub_target_texts, opensub_vocab_target, tokenizer)\n",
    "\n",
    "# anki_input_sequences = tokenize_sentences(anki_input_texts, anki_vocab_input, tokenizer)\n",
    "# anki_target_sequences = tokenize_sentences(anki_target_texts, anki_vocab_target, tokenizer)\n",
    "\n",
    "# # Паддинг последовательностей\n",
    "# opensub_input_padded = pad_sequences(opensub_input_sequences, opensub_vocab_input['<pad>'])\n",
    "# opensub_target_padded = pad_sequences(opensub_target_sequences, opensub_vocab_target['<pad>'])\n",
    "\n",
    "# anki_input_padded = pad_sequences(anki_input_sequences, anki_vocab_input['<pad>'])\n",
    "# anki_target_padded = pad_sequences(anki_target_sequences, anki_vocab_target['<pad>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832ae15-f46e-4976-85f4-ab9d17a0574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e01fc86e-215f-4387-b230-99339f1b1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Loss: 0.4616\n",
      "Iteration 2000, Loss: 0.2236\n",
      "Iteration 3000, Loss: 0.2779\n",
      "Iteration 4000, Loss: 0.3412\n",
      "Iteration 5000, Loss: 0.2966\n",
      "Iteration 6000, Loss: 0.4601\n",
      "Iteration 7000, Loss: 0.2534\n",
      "Iteration 8000, Loss: 0.2977\n",
      "Iteration 9000, Loss: 0.3404\n",
      "Iteration 10000, Loss: 0.1520\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, max_length):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Линейные преобразования для скрытого состояния и выходов кодировщика\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # Изменено на hidden_size\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))  # Изменено на hidden_size\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (1, 1, hidden_size)\n",
    "        # encoder_outputs: (input_length, hidden_size)\n",
    "\n",
    "        timestep = encoder_outputs.size(0)  # input_length\n",
    "\n",
    "        # Повторяем скрытое состояние для каждого шага во времени\n",
    "        hidden = hidden.repeat(timestep, 1, 1).transpose(0, 1)  # (1, timestep, hidden_size)\n",
    "\n",
    "        # Объединяем скрытое состояние с выходами кодировщика\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(0)  # (1, timestep, hidden_size)\n",
    "\n",
    "        # Вычисляем \"энергии\" внимания\n",
    "        attn_energies = self.score(hidden, encoder_outputs)  # (1, timestep)\n",
    "\n",
    "        # Возвращаем веса внимания\n",
    "        return torch.softmax(attn_energies, dim=1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # Объединяем скрытое состояние и выходы кодировщика\n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))  # (1, timestep, hidden_size)\n",
    "\n",
    "        # Линейная проекция\n",
    "        energy = energy.transpose(2, 1)  # (1, hidden_size, timestep)\n",
    "        v = self.v.unsqueeze(0).unsqueeze(0)  # (1, 1, hidden_size)\n",
    "\n",
    "        # Скаляное произведение для получения окончательных значений внимания\n",
    "        attn_energies = torch.bmm(v, energy)  # (1, 1, timestep)\n",
    "        return attn_energies.squeeze(1)  # (1, timestep)\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, max_length):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.attention = Attention(hidden_size, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "        \n",
    "        # Применяем внимание\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)  # (1, input_length)\n",
    "        \n",
    "        # Добавляем размерность для применения bmm\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # (1, 1, input_length)\n",
    "\n",
    "        # Вставляем размерность для encoder_outputs\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(0)  # (1, input_length, hidden_size)\n",
    "        \n",
    "        # Взвешиваем скрытые состояния кодировщика через bmm\n",
    "        context = attn_weights.bmm(encoder_outputs)  # (1, 1, hidden_size)\n",
    "\n",
    "        # Соединяем контекст и эмбеддинг\n",
    "        output = torch.cat((embedded[0], context[0]), 1)  # (1, hidden_size * 2)\n",
    "        output = self.attn_combine(output).unsqueeze(0)  # (1, 1, hidden_size)\n",
    "\n",
    "        # Пропускаем через LSTM\n",
    "        output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        output = self.softmax(self.out(output[0]))  # (1, output_size)\n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden, encoder_cell = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Создаём encoder_outputs с длиной input_length, а не max_length\n",
    "    encoder_outputs = torch.zeros(input_length, encoder.hidden_size)\n",
    "\n",
    "    # Кодирование входного тензора\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]  # Сохранение выходов кодировщика\n",
    "\n",
    "    decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])  # Токен начала\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Декодирование с вниманием\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        if decoder_input.item() == opensub_vocab_target['<eos>']:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "# Параметры\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "n_iters = 10000\n",
    "print_every = 1000\n",
    "max_length = 10  # Максимальная длина последовательности\n",
    "\n",
    "# Создание моделей\n",
    "encoder = Encoder(len(opensub_vocab_input), hidden_size)\n",
    "decoder = AttentionDecoder(len(opensub_vocab_target), hidden_size, max_length)\n",
    "\n",
    "# Оптимизаторы и функция потерь\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Цикл обучения\n",
    "for iter in range(1, n_iters + 1):\n",
    "    training_pair = [opensub_input_padded[iter % len(opensub_input_padded)], opensub_target_padded[iter % len(opensub_target_padded)]]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ccd51e8-7b87-48fd-abe3-9f012ef98c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence of 3 words: <bos> lt would be fought here ln our present <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> это\n",
      "\n",
      "Original sentence of 6 words: <bos> what the hell <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> это\n",
      "\n",
      "Original sentence of 10 words: <bos> hey what s wrong with this picture <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translated sentence: <bos> это\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создаем обратный словарь для целевого языка\n",
    "opensub_vocab_target_reverse = {index: word for word, index in opensub_vocab_target.items()}\n",
    "opensub_vocab_input_reverse = {index: word for word, index in opensub_vocab_input.items()}\n",
    "\n",
    "# Функция для выполнения перевода одного предложения\n",
    "def translate_sentence(input_tensor, encoder, decoder, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = encoder.init_hidden()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]  # Сохранение выходов кодировщика\n",
    "\n",
    "        decoder_input = torch.tensor([[opensub_vocab_target['<bos>']]])\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == opensub_vocab_target['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(opensub_vocab_target_reverse[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Пример для предложений разной длины\n",
    "sentence_lengths = [3, 6, 10]\n",
    "for length in sentence_lengths:\n",
    "    input_sentence = opensub_input_padded[length]  # Предложение из определенного количества слов\n",
    "    translated_sentence = translate_sentence(input_sentence, encoder, decoder, max_length=length)\n",
    "    print(f'Original sentence of {length} words: {\" \".join([opensub_vocab_input_reverse[word.item()] for word in input_sentence])}')\n",
    "    print(f'Translated sentence: {\" \".join(translated_sentence)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27837408-b784-43f2-aa5b-ce3c0d099167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM with Attention vector for 'happy': tensor([[-1.7619e+01, -8.0208e-04, -2.6871e+01,  ..., -1.7588e+01,\n",
      "         -1.7121e+01, -1.7231e+01]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_word_vector_lstm_attention(word, encoder, decoder, vocab, device='cpu'):\n",
    "    # Преобразование слова в тензор\n",
    "    input_tensor = torch.tensor([vocab[word]]).to(device)\n",
    "    \n",
    "    # Получение эмбеддинга слова\n",
    "    embedded = encoder.embedding(input_tensor).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "    \n",
    "    # Инициализация скрытых состояний\n",
    "    hidden, cell = encoder.init_hidden()  # (1, 1, hidden_size)\n",
    "    \n",
    "    # Прямой проход через LSTM кодировщика\n",
    "    encoder_output, (hidden, cell) = encoder.lstm(embedded, (hidden, cell))\n",
    "    \n",
    "    # Получение выходов кодировщика\n",
    "    encoder_outputs = encoder_output.squeeze(0)  # (input_length, hidden_size)\n",
    "    \n",
    "    # Создаем пустой тензор для декодера\n",
    "    decoder_input = torch.tensor([[vocab['<bos>']]]).to(device)\n",
    "    \n",
    "    # Убедитесь, что hidden и cell имеют размерность (num_layers * num_directions, batch_size, hidden_size)\n",
    "    hidden = hidden.unsqueeze(0) if hidden.dim() == 2 else hidden\n",
    "    cell = cell.unsqueeze(0) if cell.dim() == 2 else cell\n",
    "    \n",
    "    # Проходим через декодер для получения векторного представления с вниманием\n",
    "    decoder_output, _, _, _ = decoder(\n",
    "        decoder_input, hidden, cell, encoder_outputs\n",
    "    )\n",
    "    \n",
    "    # Возвращаем последнее скрытое состояние декодера\n",
    "    return decoder_output\n",
    "\n",
    "# Пример использования\n",
    "vocab = opensub_vocab_input  # Например, словарь для кодировщика\n",
    "word = 'happy'\n",
    "\n",
    "vector_lstm_attention = get_word_vector_lstm_attention(word, encoder, decoder, vocab)\n",
    "\n",
    "print(f\"LSTM with Attention vector for '{word}': {vector_lstm_attention}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573bd6e-60c5-4f8d-bcbf-3d99e9803c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5b04e-25c4-4220-b510-d8c991e45f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
