{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc42317-789f-49ab-bb9a-cbba886f63e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2024-09-12 08:20:52--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16305013 (16M) [application/zip]\n",
      "Saving to: ‘data/rus-eng.zip’\n",
      "\n",
      "data/rus-eng.zip    100%[===================>]  15.55M  2.21MB/s    in 9.7s    \n",
      "\n",
      "2024-09-12 08:21:02 (1.61 MB/s) - ‘data/rus-eng.zip’ saved [16305013/16305013]\n",
      "\n",
      "--2024-09-12 08:21:02--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 610036 (596K) [application/zip]\n",
      "Saving to: ‘data/en-ru.txt.zip’\n",
      "\n",
      "data/en-ru.txt.zip  100%[===================>] 595.74K   183KB/s    in 3.3s    \n",
      "\n",
      "2024-09-12 08:21:06 (183 KB/s) - ‘data/en-ru.txt.zip’ saved [610036/610036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget http://www.manythings.org/anki/rus-eng.zip -O 'data/rus-eng.zip'\n",
    "!wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/moses/en-ru.txt.zip -O 'data/en-ru.txt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f6b6b3-37d8-45b1-9d92-8f93ab00e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d8d022-ec2e-4b11-90f1-116e52e5eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_anki_path = 'data/rus-eng.zip'\n",
    "# with zipfile.ZipFile(dataset_anki_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('anki_data')\n",
    "# dataset_opensub_path = 'data/en-ru.txt.zip'\n",
    "# with zipfile.ZipFile(dataset_opensub_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('opensubtitles_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4e0a59-8613-4823-ba14-ed0948c661f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8a667-62c8-4b85-8e52-9b43cc10f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Zа-яёъА-ЯЁЪ.!?]+\", r\" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e51a44-8d77-489d-852a-6707cb2d6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opensubtitles_data(file_path_ru, file_path_en, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    with open(file_path_en, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text = line.strip()\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "    target_texts = []\n",
    "    with open(file_path_ru, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            ru_text = line.strip()\n",
    "            target_texts.append(normalizeString(ru_text))\n",
    "    return input_texts, target_texts\n",
    "\n",
    "def load_anki_data(file_path, num_samples=100_000):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:num_samples]:\n",
    "            en_text, ru_text = line.strip().split('\\t')[:-1]\n",
    "            input_texts.append(normalizeString(en_text))\n",
    "            target_texts.append(normalizeString(ru_text)) \n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05530c6-2939-4bd0-b576-68324ab33ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab5717-51b5-43f9-8a58-af5ac972517b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d225a3-1621-455e-bed4-dea573fabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # lines = open('anki_data/rus.txt', encoding='utf-8').\\\n",
    "    #     read().strip().split('\\n')\n",
    "\n",
    "    # pairs = [[normalizeString(s) for s in l.split('\\t')][0:2] for l in lines]\n",
    "    anki_input_texts, anki_target_texts = load_anki_data('anki_data/rus.txt')\n",
    "    opensub_input_texts, opensub_target_texts = load_opensubtitles_data('opensubtitles_data/OpenSubtitles.en-ru.ru',\n",
    "                                                                 'opensubtitles_data/OpenSubtitles.en-ru.en')\n",
    "    # pairs = list(zip(anki_input_texts, anki_target_texts)) \n",
    "    pairs = list(zip(opensub_input_texts, opensub_target_texts))\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf7f677-98ec-4e04-8155-8aab93e75a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH  and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1bfcc1-2835-453d-9771-fa0480f84a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Counted words:\n",
      "eng 999\n",
      "rus 1379\n",
      "('i m not gonna hurt anyone .', 'я никого не трону !')\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'rus', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8781adf-7aa5-42aa-8ba0-3009736e92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size,num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if str(self.rnn)[:4] == 'LSTM':\n",
    "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "                ,torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size,num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if str(self.rnn)[:4] == 'LSTM':\n",
    "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "                ,torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e8eeb6-791f-4fe9-a34f-bdb252ecc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b945ff04-3606-4596-99e1-ec6fe66769c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    # Encode input tensor\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # Decode using the encoded hidden state\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3801ee-35e5-40bc-b836-57fe62f3b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3f507e-1b91-4b14-9731-6f24cb819a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000,\n",
    "               learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print_time = time.time()\n",
    "    print_iter = 0\n",
    "    print_loss_total = 0\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter_ in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_ - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        print_iter += 1\n",
    "\n",
    "        if iter_ % 100 == 0 : \n",
    "            if (time.time() - print_time > 30) or iter_ == n_iters:\n",
    "                print_time = time.time()\n",
    "                print_loss_avg = print_loss_total / print_iter\n",
    "                print_iter = 0\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_ / n_iters),\n",
    "                                             iter_, iter_ / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "            \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4575ca4d-c161-4a09-8f9d-82eaddac592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 30s (- 63m 15s) (800 0%) 4.0931\n",
      "1m 2s (- 60m 1s) (1700 1%) 4.0058\n",
      "1m 33s (- 58m 12s) (2600 2%) 3.8576\n",
      "2m 3s (- 56m 55s) (3500 3%) 3.6404\n",
      "2m 34s (- 56m 7s) (4400 4%) 3.6211\n",
      "3m 6s (- 55m 30s) (5300 5%) 3.3818\n",
      "3m 37s (- 54m 47s) (6200 6%) 3.3225\n",
      "4m 8s (- 54m 17s) (7100 7%) 3.0963\n",
      "4m 40s (- 54m 27s) (7900 7%) 3.0186\n",
      "5m 10s (- 54m 23s) (8700 8%) 2.7887\n",
      "5m 41s (- 54m 12s) (9500 9%) 2.5852\n",
      "6m 14s (- 53m 48s) (10400 10%) 2.3835\n",
      "6m 47s (- 53m 21s) (11300 11%) 2.1350\n",
      "7m 20s (- 52m 51s) (12200 12%) 1.9401\n",
      "7m 51s (- 52m 34s) (13000 13%) 1.7572\n",
      "8m 24s (- 52m 3s) (13900 13%) 1.5513\n",
      "8m 58s (- 51m 37s) (14800 14%) 1.3746\n",
      "9m 31s (- 51m 8s) (15700 15%) 1.1503\n",
      "10m 4s (- 50m 56s) (16500 16%) 0.9757\n",
      "10m 35s (- 50m 39s) (17300 17%) 0.8460\n",
      "11m 8s (- 50m 22s) (18100 18%) 0.7226\n",
      "11m 39s (- 50m 2s) (18900 18%) 0.6368\n",
      "12m 13s (- 49m 48s) (19700 19%) 0.5571\n",
      "12m 45s (- 49m 26s) (20500 20%) 0.4512\n",
      "13m 16s (- 49m 3s) (21300 21%) 0.3981\n",
      "13m 50s (- 48m 47s) (22100 22%) 0.3546\n",
      "14m 23s (- 48m 26s) (22900 22%) 0.2846\n",
      "14m 55s (- 48m 4s) (23700 23%) 0.2607\n",
      "15m 29s (- 47m 43s) (24500 24%) 0.2465\n",
      "15m 59s (- 47m 27s) (25200 25%) 0.2414\n",
      "16m 32s (- 47m 5s) (26000 26%) 0.2414\n",
      "17m 5s (- 46m 41s) (26800 26%) 0.2333\n",
      "17m 39s (- 46m 20s) (27600 27%) 0.2114\n",
      "18m 11s (- 46m 6s) (28300 28%) 0.1605\n",
      "18m 46s (- 45m 44s) (29100 29%) 0.1912\n",
      "19m 22s (- 45m 52s) (29700 29%) 0.1986\n",
      "19m 54s (- 45m 48s) (30300 30%) 0.1921\n",
      "20m 26s (- 45m 42s) (30900 30%) 0.1687\n",
      "21m 0s (- 45m 41s) (31500 31%) 0.1881\n",
      "21m 35s (- 45m 41s) (32100 32%) 0.1630\n",
      "22m 6s (- 45m 42s) (32600 32%) 0.1703\n",
      "37m 25s (- 77m 1s) (32700 32%) 0.1767\n",
      "54m 46s (- 112m 13s) (32800 32%) 0.1180\n",
      "70m 13s (- 143m 12s) (32900 32%) 0.1612\n",
      "85m 30s (- 172m 50s) (33100 33%) 0.1786\n",
      "102m 3s (- 205m 19s) (33200 33%) 0.1285\n",
      "120m 5s (- 240m 32s) (33300 33%) 0.1736\n",
      "135m 42s (- 270m 36s) (33400 33%) 0.1786\n",
      "152m 36s (- 301m 35s) (33600 33%) 0.1848\n",
      "168m 43s (- 331m 55s) (33700 33%) 0.1010\n",
      "185m 41s (- 363m 41s) (33800 33%) 0.1621\n",
      "201m 18s (- 392m 31s) (33900 33%) 0.1395\n",
      "217m 2s (- 421m 18s) (34000 34%) 0.1911\n",
      "233m 41s (- 449m 36s) (34200 34%) 0.1148\n",
      "251m 1s (- 480m 49s) (34300 34%) 0.1517\n",
      "259m 54s (- 495m 38s) (34400 34%) 0.1672\n",
      "260m 28s (- 483m 43s) (35000 35%) 0.1489\n",
      "260m 58s (- 468m 0s) (35800 35%) 0.1478\n",
      "261m 32s (- 451m 6s) (36700 36%) 0.1576\n",
      "262m 4s (- 434m 55s) (37600 37%) 0.1601\n",
      "262m 36s (- 419m 29s) (38500 38%) 0.1253\n",
      "263m 8s (- 404m 44s) (39400 39%) 0.1291\n",
      "263m 42s (- 390m 38s) (40300 40%) 0.1497\n",
      "264m 14s (- 377m 7s) (41200 41%) 0.1418\n",
      "264m 46s (- 364m 8s) (42100 42%) 0.1570\n",
      "265m 19s (- 351m 42s) (43000 43%) 0.1001\n",
      "265m 51s (- 339m 44s) (43900 43%) 0.1275\n",
      "266m 24s (- 328m 15s) (44800 44%) 0.1263\n",
      "266m 57s (- 317m 11s) (45700 45%) 0.1543\n",
      "267m 29s (- 306m 31s) (46600 46%) 0.1192\n",
      "268m 0s (- 296m 13s) (47500 47%) 0.1435\n",
      "268m 33s (- 286m 18s) (48400 48%) 0.1104\n",
      "269m 6s (- 276m 45s) (49300 49%) 0.1147\n",
      "269m 38s (- 267m 29s) (50200 50%) 0.1441\n",
      "270m 10s (- 258m 32s) (51100 51%) 0.1307\n",
      "270m 44s (- 249m 54s) (52000 52%) 0.1194\n",
      "271m 16s (- 241m 32s) (52900 52%) 0.1586\n",
      "271m 47s (- 233m 24s) (53800 53%) 0.1347\n",
      "272m 20s (- 225m 32s) (54700 54%) 0.1057\n",
      "272m 53s (- 217m 55s) (55600 55%) 0.1422\n",
      "273m 27s (- 210m 31s) (56500 56%) 0.1023\n",
      "273m 59s (- 203m 20s) (57400 57%) 0.1161\n",
      "274m 31s (- 196m 21s) (58300 58%) 0.1107\n",
      "275m 5s (- 189m 35s) (59200 59%) 0.1341\n",
      "275m 37s (- 182m 59s) (60100 60%) 0.0926\n",
      "276m 10s (- 176m 34s) (61000 61%) 0.1188\n",
      "276m 42s (- 170m 19s) (61900 61%) 0.1194\n",
      "277m 14s (- 164m 13s) (62800 62%) 0.1253\n",
      "277m 46s (- 158m 17s) (63700 63%) 0.1577\n",
      "278m 18s (- 152m 30s) (64600 64%) 0.1210\n",
      "278m 50s (- 146m 52s) (65500 65%) 0.1234\n",
      "279m 22s (- 141m 22s) (66400 66%) 0.1050\n",
      "279m 55s (- 136m 0s) (67300 67%) 0.0909\n",
      "280m 27s (- 131m 22s) (68100 68%) 0.1575\n",
      "280m 59s (- 126m 49s) (68900 68%) 0.0878\n",
      "281m 30s (- 122m 22s) (69700 69%) 0.1094\n",
      "282m 2s (- 118m 0s) (70500 70%) 0.1393\n",
      "282m 33s (- 113m 44s) (71300 71%) 0.1080\n",
      "283m 4s (- 109m 32s) (72100 72%) 0.1460\n",
      "283m 34s (- 105m 25s) (72900 72%) 0.1290\n",
      "284m 5s (- 101m 22s) (73700 73%) 0.1227\n",
      "284m 37s (- 97m 25s) (74500 74%) 0.1108\n",
      "285m 10s (- 93m 32s) (75300 75%) 0.1070\n",
      "285m 41s (- 89m 43s) (76100 76%) 0.0851\n",
      "286m 13s (- 85m 58s) (76900 76%) 0.1062\n",
      "286m 43s (- 82m 17s) (77700 77%) 0.0887\n",
      "287m 15s (- 78m 40s) (78500 78%) 0.1132\n",
      "287m 45s (- 75m 6s) (79300 79%) 0.1033\n",
      "288m 17s (- 71m 37s) (80100 80%) 0.1050\n",
      "288m 51s (- 67m 45s) (81000 81%) 0.1014\n",
      "289m 22s (- 64m 22s) (81800 81%) 0.0989\n",
      "289m 53s (- 61m 3s) (82600 82%) 0.1062\n",
      "290m 24s (- 57m 48s) (83400 83%) 0.1419\n",
      "290m 55s (- 54m 35s) (84200 84%) 0.1176\n",
      "291m 27s (- 51m 25s) (85000 85%) 0.0923\n",
      "291m 58s (- 48m 19s) (85800 85%) 0.1236\n",
      "292m 32s (- 45m 15s) (86600 86%) 0.1182\n",
      "293m 4s (- 42m 15s) (87400 87%) 0.1049\n",
      "293m 36s (- 39m 16s) (88200 88%) 0.1244\n",
      "294m 8s (- 36m 21s) (89000 89%) 0.1134\n",
      "294m 40s (- 33m 28s) (89800 89%) 0.0999\n",
      "295m 14s (- 30m 37s) (90600 90%) 0.1065\n",
      "295m 45s (- 27m 49s) (91400 91%) 0.1062\n",
      "296m 16s (- 25m 3s) (92200 92%) 0.1233\n",
      "296m 48s (- 22m 20s) (93000 93%) 0.1001\n",
      "297m 21s (- 19m 39s) (93800 93%) 0.0952\n",
      "297m 53s (- 17m 0s) (94600 94%) 0.1011\n",
      "298m 25s (- 14m 23s) (95400 95%) 0.1115\n",
      "298m 57s (- 11m 48s) (96200 96%) 0.0796\n",
      "299m 28s (- 9m 15s) (97000 97%) 0.1081\n",
      "300m 0s (- 6m 44s) (97800 97%) 0.1195\n",
      "300m 31s (- 4m 16s) (98600 98%) 0.1324\n",
      "301m 3s (- 1m 49s) (99400 99%) 0.1243\n",
      "301m 26s (- 0m 0s) (100000 100%) 0.1162\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size,1).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words,1).to(device)\n",
    "trainIters(encoder1, decoder1, 100_000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5ffcb8-fffa-44e8-ba1e-571b85baa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words\n",
    "            \n",
    "def evaluateRandomly(encoder, decoder, l, n=3):\n",
    "    pairs_ = [pair for pair in pairs if len(pair[0].split()) == l]\n",
    "    for i in range(n):\n",
    "        \n",
    "        \n",
    "        pair = random.choice(pairs_)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[1:-1])\n",
    "        print('<', output_sentence)\n",
    "        print('')            \n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbff403a-d6ed-4813-915c-0fe975f196d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m diane selwyn .\n",
      "= меня зовут даиян селвин .\n",
      "< зовут даиян селвин .\n",
      "\n",
      "> i m not moving .\n",
      "= я никуда не поиду .\n",
      "< никуда не поиду .\n",
      "\n",
      "> she s gonna fry .\n",
      "= она просто сгорит .\n",
      "< просто сгорит .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 words\n",
    "evaluateRandomly(encoder1, decoder1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47dcdd72-eb0e-49a6-ace4-f6244a82cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> you re a prince of troy .\n",
      "= ты принц трои .\n",
      "< принц трои .\n",
      "\n",
      "> you re using her as bait .\n",
      "= ты используешь ее в качестве приманки ?\n",
      "< используешь ее в качестве приманки ?\n",
      "\n",
      "> i m gonna write it down .\n",
      "= нужно все это записать .\n",
      "< все это записать .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 words\n",
    "evaluateRandomly(encoder1, decoder1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66f018a-6e4f-4232-9729-00e50417fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> you re in prison aren t you ?\n",
      "= ты же в темнице !\n",
      "< же в темнице !\n",
      "\n",
      "> you re playinq a danqerous qame here .\n",
      "= ты затеял опасную игру .\n",
      "< затеял опасную игру .\n",
      "\n",
      "> she s been drinking too much coppertone .\n",
      "=  ты мне это говоришь ?\n",
      "< ты мне это говоришь ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 words\n",
    "evaluateRandomly(encoder1, decoder1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96bf0081-6066-4d73-9b7a-a0ca54f9fc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Слово 1 Слово 2  Косинусное расстояние       Тип\n",
      "0  страшно  ужасно              -0.165247  Синонимы\n",
      "1  страшно  уверен               0.040088  Антонимы\n"
     ]
    }
   ],
   "source": [
    "def get_word_vector(word, encoder, decoder, vocab, device='cpu'):\n",
    "    input_tensor = torch.tensor([[vocab[word]]], device=device)\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[0], encoder_hidden)\n",
    "    \n",
    "    return encoder_hidden[0].squeeze(0)\n",
    "\n",
    "def evaluate_word_pairs(encoder, decoder, vocab, synonym_pairs, antonym_pairs, device='cpu'):\n",
    "    results = []\n",
    "\n",
    "    def cosine_similarity_torch(vec1, vec2):\n",
    "        cos = F.cosine_similarity(vec1, vec2)\n",
    "        return cos.item()\n",
    "\n",
    "    for word1, word2 in synonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Синонимы\"])\n",
    "\n",
    "    for word1, word2 in antonym_pairs:\n",
    "        vec1 = get_word_vector(word1, encoder, decoder, vocab, device)\n",
    "        vec2 = get_word_vector(word2, encoder, decoder, vocab, device)\n",
    "        cosine_sim = cosine_similarity_torch(vec1, vec2)\n",
    "        results.append([word1, word2, cosine_sim, \"Антонимы\"])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Слово 1\", \"Слово 2\", \"Косинусное расстояние\", \"Тип\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "synonym_pairs = [(\"страшно\", \"ужасно\")]\n",
    "antonym_pairs = [(\"страшно\", \"уверен\")]\n",
    "\n",
    "df_results = evaluate_word_pairs(encoder1, decoder1, output_lang.word2index, synonym_pairs, antonym_pairs)\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fd086-ff42-4a5b-a6ce-19412409b2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436832c-dd51-47d9-89da-691ca6da5ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
