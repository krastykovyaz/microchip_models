{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d02f17-3406-481e-9f92-0816bad6fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  218M  100  218M    0     0  12.1M      0  0:00:18  0:00:18 --:--:-- 17.9M\n",
      "tar: Option -L is not permitted in mode -x\n"
     ]
    }
   ],
   "source": [
    "# Скачивание датасета\n",
    "!curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz --output data/flowers.tgz\n",
    "!tar -xvzf data/flowers.tgz -C data/ -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd014979-6b73-45c7-9394-74f7639e63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'data/flower_photos'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'mobilenet_v2_flowers.pth'\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b11dc1-c2cc-4b69-91a9-c287c395c7de",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eacae1c-de42-4189-9657-40d304b034df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.8037 Acc: 0.7279\n",
      "val Loss: 0.5585 Acc: 0.8120\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4802 Acc: 0.8338\n",
      "val Loss: 0.4166 Acc: 0.8556\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.4222 Acc: 0.8491\n",
      "val Loss: 0.3983 Acc: 0.8624\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.3974 Acc: 0.8597\n",
      "val Loss: 0.3905 Acc: 0.8651\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.3739 Acc: 0.8644\n",
      "val Loss: 0.4297 Acc: 0.8488\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.3656 Acc: 0.8719\n",
      "val Loss: 0.3629 Acc: 0.8719\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.3484 Acc: 0.8719\n",
      "val Loss: 0.3743 Acc: 0.8665\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.3595 Acc: 0.8736\n",
      "val Loss: 0.3497 Acc: 0.8692\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.3213 Acc: 0.8856\n",
      "val Loss: 0.3552 Acc: 0.8706\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.3572 Acc: 0.8672\n",
      "val Loss: 0.3324 Acc: 0.8801\n",
      "Модель сохранена в mobilenet_v2_flowers.pth\n"
     ]
    }
   ],
   "source": [
    "# Преобразования данных (например, изменение размера и нормализация)\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_split = 0.8\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "train_size = int(train_split * len(image_datasets))\n",
    "val_size = len(image_datasets) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_datasets, [train_size, val_size])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "num_classes = len(image_datasets.classes)\n",
    "\n",
    "# Загрузка предобученной модели MobileNetV2\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Заморозим все слои, кроме последнего\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Изменим последний классификатор под количество классов в нашем датасете\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Определяем функцию потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Включаем режим обучения\n",
    "            else:\n",
    "                model.eval()   # Включаем режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Проходим по батчам данных\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Обнуление градиентов\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только на обучении\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Обучаем модель\n",
    "model = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Сохраняем модель в файл\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f'Модель сохранена в {model_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74285f2-c500-4f91-a9ca-6841c3c997e0",
   "metadata": {},
   "source": [
    "### Inference model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0098f206-c3e8-4d63-9dec-a651625d6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_71731/3147588083.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=False)  # Создаем модель mobilenet_v2 без предобученных весов\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)  # Меняем последний классификационный слой\n",
    "model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b4d89d-878c-4f98-b753-0bb1f5a2bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8733\n",
      "CPU times: user 1min 44s, sys: 33.1 s, total: 2min 17s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Функция для выполнения инференса\n",
    "def inference(model, inputs):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():  # Отключаем градиенты для инференса\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    return preds.cpu().numpy()  # Возвращаем предсказания в формате numpy\n",
    "\n",
    "# Оценка на валидационном наборе данных\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    # Получаем предсказания модели\n",
    "    preds = inference(model, inputs)\n",
    "\n",
    "    # Сравнение предсказанных классов с истинными\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9217d-916c-40c7-b013-8c0baa47ef6e",
   "metadata": {},
   "source": [
    "### Save model to ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f56df6-9c24-4809-9284-5f7e7f34d2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_71731/1412151818.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Загружаем обученные веса\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно экспортирована в mobilenet_v2_flowers.onnx\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=False)  # Создаем MobileNetV2 без предобученных весов\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes) # Меняем последний классификационный слой\n",
    "model.load_state_dict(torch.load(model_path))  # Загружаем обученные веса\n",
    "model = model.to(device)\n",
    "model.eval()  # Устанавливаем режим оценки\n",
    "\n",
    "# Создаем фиктивный входной тензор (batch size 1, 3 канала, 224x224 пикселей)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Путь для сохранения ONNX модели\n",
    "onnx_model_path = 'mobilenet_v2_flowers.onnx'\n",
    "torch.onnx.export(model,               # Модель для экспорта\n",
    "                  dummy_input,         # Пример входного тензора\n",
    "                  onnx_model_path,     # Имя выходного файла\n",
    "                  export_params=True,  # Экспортируем обученные параметры\n",
    "                  opset_version=11,    # Версия ONNX opset\n",
    "                  do_constant_folding=True,  # Оптимизация неизменяемых частей\n",
    "                  input_names=['input'],   # Имя входного тензора\n",
    "                  output_names=['output'],  # Имя выходного тензора\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})  # Динамическое изменение размера батча\n",
    "\n",
    "print(f\"Модель успешно экспортирована в {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0d0fc-95c9-4138-8d7d-de5e87667fee",
   "metadata": {},
   "source": [
    "### Inference model in ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176292ae-b8b7-4cd4-8483-717d4da4f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f81afbe-7e53-4ca4-9d7e-0b0423930a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8828\n",
      "CPU times: user 40.9 s, sys: 4.64 s, total: 45.5 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Функция для инференса\n",
    "def onnx_inference(ort_session, inputs):\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: inputs}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    return ort_outs\n",
    "\n",
    "# Оценка на валидационном наборе данных\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    inputs_numpy = inputs.numpy()\n",
    "    outputs = onnx_inference(ort_session, inputs_numpy)\n",
    "    preds = np.argmax(outputs[0], axis=1)\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e87d44-d6cd-44da-8000-814f9bf0d440",
   "metadata": {},
   "source": [
    "### Save model in tensorflow ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e289895d-f371-4389-b62f-921431576713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilenet_v2_flowers_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilenet_v2_flowers_tf_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель экспортирована в TensorFlow формат: mobilenet_v2_flowers_tf_model\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "\n",
    "# Загружаем модель в формате ONNX\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Конвертируем ONNX модель в TensorFlow формат\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_model_path = 'mobilenet_v2_flowers_tf_model'\n",
    "tf_rep.export_graph(tf_model_path)\n",
    "print(f\"Модель экспортирована в TensorFlow формат: {tf_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421a4c9-43e0-4e4f-9c36-d9542a640862",
   "metadata": {},
   "source": [
    "### Inference model in tensorflow ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34764af-4531-4f1a-a829-324d6e48a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Загрузка модели TensorFlow\n",
    "model = tf.saved_model.load('mobilenet_v2_flowers_tf_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c94bab-b551-4ea0-8f62-a1ffc4ff9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8801\n",
      "CPU times: user 1min 6s, sys: 18.9 s, total: 1min 25s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Выполняем инференс\n",
    "def tensorflow_inference(model, inputs):\n",
    "    inputs_tf = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "    outputs = model.signatures['serving_default'](inputs_tf)['output']\n",
    "    return outputs\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    inputs_numpy = inputs.numpy()\n",
    "    outputs = tensorflow_inference(model, inputs_numpy)\n",
    "    preds = np.argmax(outputs, axis=1)\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf074a-d470-4b6c-bdf6-a6acdda4b739",
   "metadata": {},
   "source": [
    "Выводы о производительности и точности моделей:\n",
    "\n",
    "_Точность:_\n",
    "\n",
    "Оригинальная модель имеет точность на валидационной выборке 0.8733.\n",
    "Модель в формате ONNX демонстрирует лучшую точность — 0.8828, что немного выше по сравнению с оригинальной моделью.\n",
    "Модель TensorFlow ONNX также показывает хорошую точность — 0.8801, которая почти такая же, как у модели ONNX, но чуть ниже.\n",
    "Вывод: Модель в формате ONNX показала наилучшую точность среди всех трех.\n",
    "\n",
    "_Время выполнения (Wall time):_\n",
    "\n",
    "Оригинальная модель имеет время выполнения 30.9 секунд.\n",
    "Модель в формате ONNX значительно быстрее, её время выполнения составляет 16.6 секунд, что почти в два раза быстрее.\n",
    "TensorFlow ONNX модель также показала ускорение, завершив выполнение за 21.6 секунды.\n",
    "Вывод: ONNX-модели демонстрируют более быстрое время инференса по сравнению с оригинальной моделью. Особенно это заметно в случае модели ONNX, которая завершила вычисления быстрее всех.\n",
    "\n",
    "_Использование процессорных ресурсов:_\n",
    "\n",
    "Оригинальная модель требует больше пользовательского и системного времени процессора (user: 1min 44s, sys: 33.1 s).\n",
    "ONNX модель значительно экономичнее в использовании CPU ресурсов (user: 40.9 s, sys: 4.64 s).\n",
    "TensorFlow ONNX модель занимает среднее положение по времени использования CPU.\n",
    "\n",
    "\n",
    "__Рекомендация использовать ONNX формат модели Mobilenet__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb983f-b222-434b-9a3a-831a048d82e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b08fa6-0509-4322-99b2-811da45a3146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00220e-89c4-4700-b375-186a969310e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c51f76-6072-4cbb-ab42-99d74141134e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
