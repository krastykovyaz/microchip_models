{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61307306-1246-48f1-8f3a-61bf04eacd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  218M  100  218M    0     0  25.0M      0  0:00:08  0:00:08 --:--:-- 28.3M\n",
      "tar: Option -L is not permitted in mode -x\n"
     ]
    }
   ],
   "source": [
    "# Скачивание датасета\n",
    "!curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz --output data/flowers.tgz\n",
    "!tar -xvzf data/flowers.tgz -C data/ -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d6e7bf-3786-4924-8201-621b05e271ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = 'data/flower_photos'\n",
    "model_path = 'best_vgg16_flowers.pth'\n",
    "num_epochs = 10\n",
    "num_classes = 5\n",
    "learning_rate=0.001\n",
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08ca0f-3b8e-4329-baab-302f660e3a74",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eef5c7f-bd02-4563-b158-04ee802a2c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.6318 Acc: 0.7606\n",
      "val Loss: 0.4382 Acc: 0.8406\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 0.8297\n",
      "val Loss: 0.4226 Acc: 0.8515\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.4093 Acc: 0.8495\n",
      "val Loss: 0.4342 Acc: 0.8392\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.4084 Acc: 0.8501\n",
      "val Loss: 0.4269 Acc: 0.8542\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.3859 Acc: 0.8597\n",
      "val Loss: 0.4100 Acc: 0.8624\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.3709 Acc: 0.8699\n",
      "val Loss: 0.4009 Acc: 0.8638\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.3569 Acc: 0.8726\n",
      "val Loss: 0.3984 Acc: 0.8638\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.3687 Acc: 0.8600\n",
      "val Loss: 0.3931 Acc: 0.8733\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.3569 Acc: 0.8678\n",
      "val Loss: 0.3918 Acc: 0.8624\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.3850 Acc: 0.8566\n",
      "val Loss: 0.3962 Acc: 0.8747\n"
     ]
    }
   ],
   "source": [
    "# Преобразования данных (например, изменение размера и нормализация)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_split = 0.8\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "train_size = int(train_split * len(image_datasets))\n",
    "val_size = len(image_datasets) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_datasets, [train_size, val_size])\n",
    "\n",
    "# Создаем DataLoader'ы для обучения и валидации\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "num_classes = len(image_datasets.classes)\n",
    "\n",
    "# Загрузка предобученной модели VGG-16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Заморозим все слои, кроме последнего\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Изменим последний классификатор под количество классов в нашем датасете\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Определяем функцию потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier[6].parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "    \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Включаем режим обучения\n",
    "            else:\n",
    "                model.eval()   # Включаем режим оценки\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "    \n",
    "            # Проходим по батчам данных\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "    \n",
    "                # Обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "    \n",
    "                    # Обратное распространение и оптимизация только на обучении\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "    \n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Обучаем модель\n",
    "model = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Сохраняем модель в файл\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53aad0-ee2b-4a30-9807-8b65601e73ed",
   "metadata": {},
   "source": [
    "### Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe71a9c-445c-47c7-8848-13594a5c3aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_70488/3127508217.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=False)  # Создаем модель VGG-16 без предобученных весов\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)  # Меняем последний классификационный слой\n",
    "model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc73c8b-ee09-484c-bc1c-222437bbec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8706\n",
      "CPU times: user 11min 13s, sys: 1min 38s, total: 12min 52s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Функция для выполнения инференса\n",
    "def inference(model, inputs):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():  # Отключаем градиенты для инференса\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    return preds.cpu().numpy()  # Возвращаем предсказания в формате numpy\n",
    "\n",
    "# Оценка на валидационном наборе данных\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    # Получаем предсказания модели\n",
    "    preds = inference(model, inputs)\n",
    "\n",
    "    # Сравнение предсказанных классов с истинными\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d189c-1fad-4d65-ba49-230066d64739",
   "metadata": {},
   "source": [
    "### Save model to ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41dcdde1-5990-4335-a233-12ca13f47edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_70488/2472523256.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель VGG-16 успешно экспортирована в vgg16_flowers.onnx\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=False)  # Создаем модель VGG-16 без предобученных весов\n",
    "model.classifier[6] = torch.nn.Linear(4096, num_classes)  # Меняем последний классификационный слой\n",
    "model.load_state_dict(torch.load(model_path)) # Загружаем обученные веса\n",
    "model = model.to(device)\n",
    "model.eval()  # Устанавливаем режим оценки\n",
    "\n",
    "# Создаем фиктивный входной тензор (batch size 1, 3 канала, 224x224 пикселей)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Путь для сохранения ONNX модели\n",
    "onnx_model_path = 'vgg16_flowers.onnx'\n",
    "\n",
    "# Экспортируем модель в формат ONNX\n",
    "torch.onnx.export(\n",
    "    model,                     # Модель для экспорта\n",
    "    dummy_input,               # Пример входного тензора\n",
    "    onnx_model_path,           # Имя выходного файла\n",
    "    export_params=True,        # Экспортируем обученные параметры\n",
    "    opset_version=11,          # Версия ONNX opset\n",
    "    do_constant_folding=True,  # Оптимизация неизменяемых частей\n",
    "    input_names=['input'],     # Имя входного тензора\n",
    "    output_names=['output'],   # Имя выходного тензора\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})  # Динамическое изменение размера батча\n",
    "\n",
    "print(f\"Модель VGG-16 успешно экспортирована в {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a00755-9a39-4554-abc7-7d6af468840e",
   "metadata": {},
   "source": [
    "### Inference model in ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982e1a22-d3ee-45a6-8b7e-f1d0a54fbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd1953d-9aff-40a8-ae9c-e5d4ec4d5968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8610\n",
      "CPU times: user 14min 34s, sys: 1min, total: 15min 35s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Функция для инференса\n",
    "def onnx_inference(ort_session, inputs):\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: inputs}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    return ort_outs\n",
    "\n",
    "# Оценка на валидационном наборе данных\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    inputs_numpy = inputs.numpy()\n",
    "    outputs = onnx_inference(ort_session, inputs_numpy)\n",
    "    preds = np.argmax(outputs[0], axis=1)\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865431b2-7d54-40d1-9676-b5231531f8b1",
   "metadata": {},
   "source": [
    "### Save model in tensorflow ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09bdb5d-ddda-4913-a221-1169a7950b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/micro_llm/venv/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_53_x, mul_3_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg16_flowers_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg16_flowers_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to vgg16_flowers_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель экспортирована в TensorFlow формат: vgg16_flowers_tf_model\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "\n",
    "# Загружаем модель в формате ONNX\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Конвертируем ONNX модель в TensorFlow формат\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_model_path = 'vgg16_flowers_tf_model'\n",
    "tf_rep.export_graph(tf_model_path)\n",
    "print(f\"Модель экспортирована в TensorFlow формат: {tf_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34518277-da52-461d-ac31-207a3340f3dc",
   "metadata": {},
   "source": [
    "### Inference model in tensorflow ONXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95f062c-69ac-4185-a9e5-34d385aa768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Загрузка модели TensorFlow\n",
    "model = tf.saved_model.load(tf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d433bb-7504-47f9-b265-605f98b86aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.8569\n",
      "CPU times: user 17min 53s, sys: 1min 49s, total: 19min 42s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Выполняем инференс\n",
    "def tensorflow_inference(model, inputs):\n",
    "    inputs_tf = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "    outputs = model.signatures['serving_default'](inputs_tf)['output']\n",
    "    return outputs\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for inputs, labels in dataloaders['val']:\n",
    "    inputs_numpy = inputs.numpy()\n",
    "    outputs = tensorflow_inference(model, inputs_numpy)\n",
    "    preds = np.argmax(outputs, axis=1)\n",
    "    correct += np.sum(preds == labels.numpy())\n",
    "    total += labels.size(0)\n",
    "\n",
    "# Выводим точность модели\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on validation dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "744d8050-2962-42da-862c-9a15dee813a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_70488/1224870059.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before pruning: 0.8638\n",
      "Pruning 10.0% - Accuracy: 0.8556\n",
      "Pruning 20.0% - Accuracy: 0.8460\n",
      "Pruning 30.0% - Accuracy: 0.8542\n",
      "Pruning 40.0% - Accuracy: 0.8597\n",
      "Pruning 50.0% - Accuracy: 0.8065\n",
      "Pruning 60.0% - Accuracy: 0.5845\n",
      "Pruning 70.0% - Accuracy: 0.2657\n",
      "Training accuracy fell below 40% at 70.0% sparsity.\n",
      "Pruning 80.0% - Accuracy: 0.2207\n",
      "Training accuracy fell below 40% at 80.0% sparsity.\n",
      "Pruning 90.0% - Accuracy: 0.2534\n",
      "Training accuracy fell below 40% at 90.0% sparsity.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Функция для применения обрезки к модели\n",
    "def prune_model(model, pruning_percentage):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):  # Применяем обрезку только к слоям nn.Linear\n",
    "            prune.l1_unstructured(module, name='weight', amount=pruning_percentage)\n",
    "    return model\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloaders['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "    accuracy = correct.double() / total\n",
    "    return accuracy.item()\n",
    "\n",
    "# Загружаем сохраненные веса модели\n",
    "pruning_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Уровни разреженности\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "# Оценка исходной модели до обрезки\n",
    "initial_accuracy = evaluate_model(model)\n",
    "print(f'Accuracy before pruning: {initial_accuracy:.4f}')\n",
    "\n",
    "# Применяем обрезку и оцениваем модель на разных уровнях sparsity\n",
    "for pruning_percentage in pruning_percentages:\n",
    "    pruned_model = prune_model(model, pruning_percentage)\n",
    "    pruned_accuracy = evaluate_model(pruned_model)\n",
    "    print(f'Pruning {pruning_percentage * 100}% - Accuracy: {pruned_accuracy:.4f}')\n",
    "    if pruned_accuracy < 0.4:\n",
    "        print(f'Training accuracy fell below 40% at {pruning_percentage * 100}% sparsity.')\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfe4bc-960e-435a-be81-40eee8612cb3",
   "metadata": {},
   "source": [
    "Pruning 10% - Accuracy: 0.85 — на начальном этапе точность даже немного улучшилась после обрезки. Это может объясняться тем, что небольшая разреженность помогает модели избавиться от менее значимых весов, что предотвращает переобучение.\n",
    "Pruning 20% - Accuracy: 0.86 и Pruning 30% - Accuracy: 0.85 и Pruning 40% - Accuracy: 0.85 —  Модель теряет часть весов, но все еще способна делать хорошие предсказания.\n",
    "Pruning 40% - Accuracy: 0.85 — точность остается стабильной. До этого порога модель сохраняет свою структуру, несмотря на обрезку до 40%. </br>\n",
    "Pruning 50% - Accuracy: 0.82 и Pruning 60% - Accuracy: 0.56 — с увеличением уровня разреженности точность начинает заметно снижаться, так как важные параметры начинают обнуляться, что ухудшает способность модели делать точные предсказания.\n",
    "Pruning 70% - Accuracy: 0.25 — модель практически теряет способность правильно классифицировать объекты. </br>\n",
    "_Точность обучения падает ниже 40% при разреженности 70%, что делает модель практически бесполезной._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4a2f517-cd05-4c48-97e0-f372602be108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_70488/3386099547.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Загрузка модели\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.9005, Target accuracy: 0.8105\n",
      "Layer: features.0, Sparsity: 5.00%, Accuracy: 0.9033\n",
      "Layer: features.0, Sparsity: 10.00%, Accuracy: 0.8965\n",
      "Layer: features.0, Sparsity: 15.00%, Accuracy: 0.8896\n",
      "Layer: features.0, Sparsity: 20.00%, Accuracy: 0.8597\n",
      "Layer: features.0, Sparsity: 25.00%, Accuracy: 0.7929\n",
      "Layer: features.2, Sparsity: 5.00%, Accuracy: 0.7752\n",
      "Layer: features.5, Sparsity: 5.00%, Accuracy: 0.7861\n",
      "Layer: features.7, Sparsity: 5.00%, Accuracy: 0.7807\n",
      "Layer: features.10, Sparsity: 5.00%, Accuracy: 0.7861\n",
      "Layer: features.12, Sparsity: 5.00%, Accuracy: 0.7847\n",
      "Layer: features.14, Sparsity: 5.00%, Accuracy: 0.7316\n",
      "Layer: features.17, Sparsity: 5.00%, Accuracy: 0.7234\n",
      "Layer: features.19, Sparsity: 5.00%, Accuracy: 0.7262\n",
      "Layer: features.21, Sparsity: 5.00%, Accuracy: 0.6921\n",
      "Layer: features.24, Sparsity: 5.00%, Accuracy: 0.7057\n",
      "Layer: features.26, Sparsity: 5.00%, Accuracy: 0.6866\n",
      "Layer: features.28, Sparsity: 5.00%, Accuracy: 0.6635\n",
      "Layer: classifier.0, Sparsity: 5.00%, Accuracy: 0.6853\n",
      "Layer: classifier.3, Sparsity: 5.00%, Accuracy: 0.6567\n",
      "Layer: classifier.6, Sparsity: 5.00%, Accuracy: 0.6649\n",
      "Sparsity values for each layer:\n",
      "features.0: 20.00%\n",
      "features.2: 0.00%\n",
      "features.5: 0.00%\n",
      "features.7: 0.00%\n",
      "features.10: 0.00%\n",
      "features.12: 0.00%\n",
      "features.14: 0.00%\n",
      "features.17: 0.00%\n",
      "features.19: 0.00%\n",
      "features.21: 0.00%\n",
      "features.24: 0.00%\n",
      "features.26: 0.00%\n",
      "features.28: 0.00%\n",
      "classifier.0: 0.00%\n",
      "classifier.3: 0.00%\n",
      "classifier.6: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели VGG-16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Настройка данных\n",
    "data_dir = 'data/flower_photos'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'best_vgg16_flowers.pth'\n",
    "num_classes = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Преобразования данных\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Загрузка датасета и разделение на тренировочные и валидационные данные\n",
    "train_split = 0.8\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "train_size = int(train_split * len(image_datasets))\n",
    "val_size = len(image_datasets) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_datasets, [train_size, val_size])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier[6].parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "    return correct.double() / total\n",
    "\n",
    "\n",
    "layer_sparsity = {}\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "initial_accuracy = evaluate_model(model, dataloaders)\n",
    "target_accuracy = initial_accuracy * 0.9\n",
    "\n",
    "print(f'Initial accuracy: {initial_accuracy:.4f}, Target accuracy: {target_accuracy:.4f}')\n",
    "\n",
    "\n",
    "pruned_model = copy.deepcopy(model)\n",
    "\n",
    "# Применение prunining для каждого слоя\n",
    "for name, module in pruned_model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        current_sparsity = 0.0\n",
    "        layer_accuracy = initial_accuracy\n",
    "        \n",
    "        while layer_accuracy >= target_accuracy:\n",
    "            current_sparsity += 0.05  # Увеличиваем sparsity на 5% за шаг\n",
    "            \n",
    "            # Применяем прунинг к весам текущего слоя\n",
    "            prune.ln_structured(module, name='weight', amount=current_sparsity, n=2, dim=0)\n",
    "            layer_accuracy = evaluate_model(pruned_model, dataloaders)\n",
    "            print(f'Layer: {name}, Sparsity: {current_sparsity*100:.2f}%, Accuracy: {layer_accuracy:.4f}')\n",
    "            if layer_accuracy < target_accuracy:\n",
    "                # Если точность упала ниже целевого значения, остановка прунинга этого слоя\n",
    "                layer_sparsity[name] = current_sparsity - 0.05  # Сохраняем последнее валидное значение sparsity\n",
    "                break\n",
    "\n",
    "print(\"Sparsity values for each layer:\")\n",
    "for layer, sparsity in layer_sparsity.items():\n",
    "    print(f'{layer}: {sparsity*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22503ffc-37b5-4b97-be07-e03705d14e73",
   "metadata": {},
   "source": [
    "Прунинг был эффективен с уменьшением точности до нужного уровня при sparsity в 20%. </br>\n",
    "Для слоев начиная с features.2 и до конца прунинг не был применен, так как при первых тестах sparsity в 5% сразу значительно снижало точность ниже целевого порога (0.8105).  </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d1baaa-06da-42e9-b729-e440547c2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/cz5kycc54pq7926rs5ddp6t00000gp/T/ipykernel_70488/2311144916.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before pruning: 0.8896\n",
      "\n",
      "Applying 10.0% pruning...\n",
      "Epoch 0/2, Loss: 1.9222, Accuracy: 0.3147\n",
      "Epoch 1/2, Loss: 1.3048, Accuracy: 0.4292\n",
      "Epoch 2/2, Loss: 1.2551, Accuracy: 0.4707\n",
      "Pruning 10.0% - Accuracy after fine-tuning: 0.4728\n",
      "\n",
      "Applying 20.0% pruning...\n",
      "Epoch 0/2, Loss: 1.2211, Accuracy: 0.4772\n",
      "Epoch 1/2, Loss: 1.2112, Accuracy: 0.4922\n",
      "Epoch 2/2, Loss: 1.1675, Accuracy: 0.5037\n",
      "Pruning 20.0% - Accuracy after fine-tuning: 0.5313\n",
      "\n",
      "Applying 30.0% pruning...\n",
      "Epoch 0/2, Loss: 1.1834, Accuracy: 0.5082\n",
      "Epoch 1/2, Loss: 1.0990, Accuracy: 0.5589\n",
      "Epoch 2/2, Loss: 1.1205, Accuracy: 0.5545\n",
      "Pruning 30.0% - Accuracy after fine-tuning: 0.5381\n",
      "\n",
      "Applying 40.0% pruning...\n",
      "Epoch 0/2, Loss: 1.1186, Accuracy: 0.5518\n",
      "Epoch 1/2, Loss: 1.0865, Accuracy: 0.5671\n",
      "Epoch 2/2, Loss: 1.0702, Accuracy: 0.5777\n",
      "Pruning 40.0% - Accuracy after fine-tuning: 0.5627\n",
      "\n",
      "Applying 50.0% pruning...\n",
      "Epoch 0/2, Loss: 1.0566, Accuracy: 0.5916\n",
      "Epoch 1/2, Loss: 1.0541, Accuracy: 0.5807\n",
      "Epoch 2/2, Loss: 1.0376, Accuracy: 0.5834\n",
      "Pruning 50.0% - Accuracy after fine-tuning: 0.5899\n",
      "\n",
      "Applying 60.0% pruning...\n",
      "Epoch 0/2, Loss: 0.9581, Accuracy: 0.6219\n",
      "Epoch 1/2, Loss: 0.9536, Accuracy: 0.6192\n",
      "Epoch 2/2, Loss: 0.9385, Accuracy: 0.6298\n",
      "Pruning 60.0% - Accuracy after fine-tuning: 0.6104\n",
      "\n",
      "Applying 70.0% pruning...\n",
      "Epoch 0/2, Loss: 0.9994, Accuracy: 0.6250\n",
      "Epoch 1/2, Loss: 0.9233, Accuracy: 0.6359\n",
      "Epoch 2/2, Loss: 0.9300, Accuracy: 0.6410\n",
      "Pruning 70.0% - Accuracy after fine-tuning: 0.6349\n",
      "\n",
      "Applying 80.0% pruning...\n",
      "Epoch 0/2, Loss: 1.1703, Accuracy: 0.5419\n",
      "Epoch 1/2, Loss: 1.0166, Accuracy: 0.6182\n",
      "Epoch 2/2, Loss: 0.9713, Accuracy: 0.6182\n",
      "Pruning 80.0% - Accuracy after fine-tuning: 0.6294\n",
      "\n",
      "Applying 90.0% pruning...\n",
      "Epoch 0/2, Loss: 1.5175, Accuracy: 0.2435\n",
      "Epoch 1/2, Loss: 1.4645, Accuracy: 0.2435\n",
      "Epoch 2/2, Loss: 1.4248, Accuracy: 0.2616\n",
      "Pruning 90.0% - Accuracy after fine-tuning: 0.3324\n",
      "Training accuracy fell below 40% at 90.0% sparsity.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "\n",
    "# Функция для применения обрезки к модели\n",
    "def prune_model(model, pruning_percentage):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):  # Применяем обрезку только к слоям nn.Linear\n",
    "            prune.l1_unstructured(module, name='weight', amount=pruning_percentage)\n",
    "    return model\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloaders['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "    accuracy = correct.double() / total\n",
    "    return accuracy.item()\n",
    "\n",
    "# Функция для дообучения модели после обрезки\n",
    "def fine_tune_model(model, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects.double() / total\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Загрузка модели и ее подготовка к прунингу\n",
    "pruning_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Уровни разреженности\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "\n",
    "# Оценка исходной модели до обрезки\n",
    "initial_accuracy = evaluate_model(model)\n",
    "print(f'Accuracy before pruning: {initial_accuracy:.4f}')\n",
    "\n",
    "# Определение функции потерь и оптимизатора для дообучения\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Применяем обрезку, дообучаем и оцениваем модель на разных уровнях sparsity\n",
    "for pruning_percentage in pruning_percentages:\n",
    "    print(f'\\nApplying {pruning_percentage * 100}% pruning...')\n",
    "    pruned_model = prune_model(model, pruning_percentage)\n",
    "\n",
    "    # Размораживаем параметры модели для дообучения\n",
    "    for param in pruned_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Дообучаем модель после прунинга\n",
    "    pruned_model = fine_tune_model(pruned_model, criterion, optimizer, num_epochs=3)\n",
    "    pruned_accuracy = evaluate_model(pruned_model)\n",
    "    print(f'Pruning {pruning_percentage * 100}% - Accuracy after fine-tuning: {pruned_accuracy:.4f}')\n",
    "    if pruned_accuracy < 0.4:\n",
    "        print(f'Training accuracy fell below 40% at {pruning_percentage * 100}% sparsity.')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f47e66-63c8-4a06-bf48-fb9f378463ff",
   "metadata": {},
   "source": [
    "Удалось достичь максимального уровня разреженности в **70%** с сохранением приемлемого качества модели. При 70% sparsity точность модели после fine-tuning составила **63.49%**, что является наивысшим значением среди всех уровней sparsity до 80%. </br>\n",
    "\n",
    "Однако, начиная с **80%** sparsity, точность начала снижаться, и при 90% sparsity точность модели резко упала до **33.24%**, что значительно ниже требуемого порога в 40%. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1ed0f-8e1e-44b8-87ca-f3c18a0c5d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
